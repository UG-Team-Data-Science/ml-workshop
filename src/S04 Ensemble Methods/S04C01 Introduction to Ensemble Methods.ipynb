{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e8bb5b",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction to Ensemble Methods\n",
    "\n",
    "Ensemble methods are a powerful class of machine learning techniques that combine multiple models to improve predictive performance. The main idea is to leverage the strengths of different models while mitigating their weaknesses, leading to more robust and accurate predictions.\n",
    "\n",
    "Ensemble methods can be broadly categorized into two types:\n",
    "\n",
    "-   **Bagging (Bootstrap Aggregating)**: Combines predictions from multiple models trained on different subsets of the training data. The most common example is Random Forests, which builds multiple decision trees and averages their predictions.\n",
    "-   **Boosting**: Sequentially builds models, where each new model focuses on correcting the errors made by the previous models. Examples include AdaBoost, Gradient Boosting, and XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a5787",
   "metadata": {},
   "source": [
    "## Why Use Ensemble Methods?\n",
    "\n",
    "-   **Improved Accuracy**: By combining multiple models, ensemble methods can achieve higher accuracy than individual models, especially in complex datasets.\n",
    "-   **Robustness**: Ensembles are less sensitive to noise and outliers, as they average out the errors of individual models.\n",
    "-   **Reduced Overfitting**: Bagging methods like Random Forests reduce overfitting by averaging predictions from multiple trees, while boosting methods focus on correcting errors rather than fitting noise.\n",
    "-   **Flexibility**: Ensemble methods can be applied to a wide range of base models, including decision trees, linear models, and neural networks, making them versatile for various tasks.\n",
    "-   **Interpretability**: Some ensemble methods, like Random Forests, provide insights into feature importance, helping to understand which features contribute most to the predictions.\n",
    "-   **Scalability**: Many ensemble methods can be parallelized, making them suitable for large datasets and high-dimensional feature spaces.\n",
    "-   **Handling Imbalanced Data**: Ensemble methods can be effective in dealing with imbalanced datasets by focusing on the minority class during training, especially in boosting methods.\n",
    "-   **Combining Different Algorithms**: Ensembles can combine predictions from different types of models, allowing for a more comprehensive understanding of the data and improving overall performance.\n",
    "-   **Hyperparameter Tuning**: Ensembles can help in hyperparameter tuning by providing a more stable performance across different configurations, as the ensemble averages out the variability of individual models.\n",
    "-   **Feature Selection**: Some ensemble methods, like Random Forests, can automatically perform feature selection by evaluating feature importance, helping to reduce dimensionality and improve model interpretability.\n",
    "-   **Real-World Applications**: Ensemble methods are widely used in various domains, including finance (credit scoring), healthcare (disease prediction), and image recognition, due to their effectiveness in handling complex and noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f6b5c3",
   "metadata": {},
   "source": [
    "## Practical Demonstration\n",
    "\n",
    "To illustrate the concepts of ensemble methods, we will use the Iris dataset and apply both Random Forests and Gradient Boosting classifiers. We will compare their performance using accuracy and confusion matrices.\n",
    "\n",
    "-   Loading the `iris` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e5089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b92e3",
   "metadata": {},
   "source": [
    "-   Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7908436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73739028",
   "metadata": {},
   "source": [
    "-   Train a Random Forest Classifier and evaluate its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c0b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Classifier Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf, target_names=class_names))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(rf_model, X_test, y_test,\n",
    "                                      display_labels=class_names, cmap='Blues')\n",
    "plt.title('Random Forest Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1108bd",
   "metadata": {},
   "source": [
    "-   Train a Gradient Boosting Classifier and evaluate its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5d19ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "print(\"Gradient Boosting Classifier Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
    "print(classification_report(y_test, y_pred_gb, target_names=class_names))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(gb_model, X_test, y_test,\n",
    "                                      display_labels=class_names, cmap='Blues')\n",
    "plt.title('Gradient Boosting Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08ead77",
   "metadata": {},
   "source": [
    "-   Feature importance for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063be6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_model.feature_importances_\n",
    "indices = importances.argsort()[::-1]\n",
    "print(\"Feature importances for Random Forest:\")\n",
    "for i in range(X.shape[1]):\n",
    "    print(f\"{feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X.shape[1]), importances[indices], align='center')\n",
    "plt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=45)\n",
    "plt.title('Feature Importances from Random Forest')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177f61eb",
   "metadata": {},
   "source": [
    "-   Feature importance for Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fda268",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "gb_importances = gb_model.feature_importances_\n",
    "gb_indices = gb_importances.argsort()[::-1]\n",
    "print(\"Feature importances for Gradient Boosting:\")\n",
    "for i in range(X.shape[1]):\n",
    "    print(f\"{feature_names[gb_indices[i]]}: {gb_importances[gb_indices[i]]:.4f}\")\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X.shape[1]), gb_importances[gb_indices], align='center')\n",
    "plt.xticks(range(X.shape[1]), [feature_names[i] for i in gb_indices], rotation=45)\n",
    "plt.title('Feature Importances from Gradient Boosting')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a74f9f",
   "metadata": {},
   "source": [
    "## Hands-on Exercises\n",
    "\n",
    "**Voting Classifier**: Implement a Voting Classifier that combines the predictions of the Random Forest and Gradient Boosting classifiers. Evaluate its performance on the test set.\n",
    "\n",
    "-   Import, instantiate, and train a `VotingClassifier` model from `sklearn.ensemble`.\n",
    "\n",
    "-   Evaluate the performance of the Voting Classifier"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "ML Workshop",
   "language": "python",
   "name": "ml_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
