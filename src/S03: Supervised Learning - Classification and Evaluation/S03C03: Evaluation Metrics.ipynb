{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d7f795",
   "metadata": {},
   "source": [
    "\n",
    "# Evaluation Metrics for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4708bd9",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "In classification tasks, we often need to evaluate how well our model performs. The ****confusion matrix**** is a powerful tool that summarizes the performance of a classification algorithm by comparing predicted and actual labels.\n",
    "\n",
    "|                | **Predicted = 1**   | **Predicted = 0**   |\n",
    "|-------------- |------------------- |------------------- |\n",
    "| **Actual = 1** | True Positive (TP)  | False Negative (FN) |\n",
    "| **Actual = 0** | False Positive (FP) | True Negative (TN)  |\n",
    "\n",
    "From these four numbers we obtain the core metrics summarized below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f0b801",
   "metadata": {},
   "source": [
    "## Accuracy (Overall Success Rate)\n",
    "\n",
    "Accuracy answers: \"What fraction of **all** predictions are correct?\"\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{FN} + \\text{TN}}$$\n",
    "\n",
    "High accuracy is desirable, but it can be ****inflated**** when one class dominates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b974e8ad",
   "metadata": {},
   "source": [
    "## Precision (Positive Predictive Value)\n",
    "\n",
    "Precision answers: \"When the classifier predicts 1, how often is it correct?\"\n",
    "\n",
    "$$\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$$\n",
    "\n",
    "High precision $\\Rightarrow$ few false alarms, but it says nothing about missed positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb56a21",
   "metadata": {},
   "source": [
    "## Recall (Sensitivity, True-Positive Rate)\n",
    "\n",
    "Recall answers: \"Of all the actual 1 cases, how many did we catch?\"\n",
    "\n",
    "$$\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$$\n",
    "\n",
    "High recall $\\Rightarrow$ few misses, but it does not guard against many false alarms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7567c4a7",
   "metadata": {},
   "source": [
    "## F1-Score\n",
    "\n",
    "The ****F1-score**** is the harmonic mean of precision and recall, balancing the two:\n",
    "\n",
    "$$F_1 = 2 \\cdot \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "It is 1 only when both precision and recall are 1; it drops sharply when either is low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ed3d72",
   "metadata": {},
   "source": [
    "## Precision-Recall Trade-off\n",
    "\n",
    "The precision-recall trade-off is a key concept in classification. By adjusting the decision threshold, we can increase precision at the cost of recall or vice versa. This trade-off is often visualized using the **precision-recall curve**, which plots precision against recall for different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb1d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
    "\n",
    "# Generating a synthetic and train a Logistic Regression model\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.linspace(0, 10, 1001).reshape(-1, 1)\n",
    "y = (X[:, 0] > 5).astype(int)  # Binary target based on a threshold\n",
    "\n",
    "# Introduce some noise so that not all points are perfectly separable\n",
    "noise = np.random.normal(0, 0.35, size=y.shape)\n",
    "y = (y + noise > 0.5).astype(int)  # Add noise to the target\n",
    "\n",
    "# Plotting the dataset\n",
    "plt.scatter(X, y, color='blue', alpha=0.5)\n",
    "plt.title(\"Synthetic Dataset\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c6755",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model accuracy:\", accuracy)\n",
    "\n",
    "# Plotting the Precision-Recall curve\n",
    "display = PrecisionRecallDisplay.from_estimator(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097768c2",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic (ROC) Curve\n",
    "\n",
    "The ROC curve visualizes the trade-off between true positive rate (recall) and false positive rate (FPR) across different thresholds. The area under the ROC curve (AUC) quantifies the model's ability to distinguish between classes. AUC = 0.5 means random guessing, while AUC = 1 means perfect classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7bba05",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, RocCurveDisplay\n",
    "display = RocCurveDisplay.from_estimator(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd80704",
   "metadata": {},
   "source": [
    "## Area Under the Curve (AUC)\n",
    "\n",
    "The ****AUC**** (Area Under the Curve) is a single scalar value summarizing the ROC curve. It represents the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance. AUC = 0.5 indicates no discrimination ability, while AUC = 1 indicates perfect discrimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb08955e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "print(f\"AUC: {auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c8f82f",
   "metadata": {},
   "source": [
    "## Practical Demonstration\n",
    "\n",
    "Below we create a simple synthetic binary dataset, train a classifier, and compute all four metrics.\n",
    "\n",
    "-   Generating a synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3421ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3,\n",
    "                           n_redundant=0, n_clusters_per_class=1,\n",
    "                           flip_y=0.02, class_sep=1.0, random_state=42)\n",
    "df = pd.DataFrame(X, columns=[f\"feature{i}\" for i in range(1, 6)])\n",
    "df[\"target\"] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b28ec",
   "metadata": {},
   "source": [
    "-   Training/test split & model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7233ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(df.drop(columns=[\"target\"]), df[\"target\"],\n",
    "                     test_size=0.25, random_state=0, stratify=df[\"target\"])\n",
    "\n",
    "clf = LogisticRegression(max_iter=500)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c275519",
   "metadata": {},
   "source": [
    "-   Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2537744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    ConfusionMatrixDisplay, precision_recall_curve,\n",
    "    accuracy_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Confusion matrix at the default 0.5 threshold\n",
    "y_pred = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# Detailed report includes precision, recall, f1 and support\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# Visualise confusion matrix\n",
    "disp = ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test,\n",
    "                                             cmap='coolwarm')\n",
    "plt.title(\"Confusion Matrix (threshold = 0.5)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dea2626",
   "metadata": {},
   "source": [
    "-   Precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e2d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "display = PrecisionRecallDisplay.from_estimator(clf, X_test, y_test)\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e614a1",
   "metadata": {},
   "source": [
    "-   ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be75d59f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "display = RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed35aa",
   "metadata": {},
   "source": [
    "## Hands-on Exercises\n",
    "\n",
    "-   **Threshold Tuning**: Try several probability thresholds and observe how accuracy, precision, recall and F1 change.\n",
    "\n",
    "-   **Class-Imbalance Scenario**:\n",
    "    -   Generate a highly unbalanced dataset (e.g. 95% zeros, 5% ones); check the documentation of `make_classification`.\n",
    "    -   Train the same model and compute the metrics again.\n",
    "    -   Plot the **precision-recall curve** and choose a better threshold for the minority class.\n",
    "\n",
    "-   **Multi-class Extension**\n",
    "    -   Use the [classification\\_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) on the Iris dataset and compare **macro** vs **weighted** F1.\n",
    "    -   Plot the confusion matrix for the Iris dataset."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "ML Workshop",
   "language": "python",
   "name": "ml_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
