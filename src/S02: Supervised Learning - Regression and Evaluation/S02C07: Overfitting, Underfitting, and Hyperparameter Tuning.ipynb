{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf7ff470",
   "metadata": {},
   "source": [
    "\n",
    "# Overfitting, Underfitting & Hyperparameter Tuning\n",
    "\n",
    "We've already seen how to build regression models, and briefly touched on the concepts of overfitting and underfitting. In this section, we will explore these concepts in more detail, along with hyperparameter tuning techniques to improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f364e52e",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting\n",
    "\n",
    "Overfitting and underfitting are common issues in machine learning models, particularly in regression tasks.\n",
    "\n",
    "-   **Underfitting** occurs when a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and test datasets.\n",
    "-   **Overfitting** occurs when a model is too complex, capturing noise in the training data rather than the underlying pattern, leading to poor generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db08e296",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "\n",
    "The bias-variance tradeoff is a key concept in understanding overfitting and underfitting:\n",
    "\n",
    "-   ****Bias****: Error due to overly simplistic assumptions in the learning algorithm. High bias leads to underfitting.\n",
    "-   ****Variance****: Error due to excessive sensitivity to small fluctuations in the training set. High variance leads to overfitting.\n",
    "-   ****Tradeoff****: A good model balances bias and variance to minimize total error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4776e796",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "\n",
    "Learning curves are a useful tool to visualize the performance of a model as the size of the training dataset increases. They can help identify overfitting and underfitting:\n",
    "\n",
    "-   ****Underfitting****: High training and validation errors, indicating the model is too simple.\n",
    "-   ****Overfitting****: Low training error but high validation error, indicating the model is too complex and captures noise in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35242027",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Calculate learning curves\n",
    "train_sizes, train_scores, test_scores = learning_curve(model, X_train, y_train, cv=5, n_jobs=-1,\n",
    "                                                        train_sizes=np.linspace(0.1, 1.0, 20),\n",
    "                                                        scoring='r2')\n",
    "\n",
    "# Calculate mean and standard deviation of training and test scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Learning Curves (Linear Regression)\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid()\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc45fa46",
   "metadata": {},
   "source": [
    "## Visualizing Overfitting and Underfitting\n",
    "\n",
    "We can visualize overfitting and underfitting using polynomial regression. A simple linear model may underfit, while a high-degree polynomial may overfit the data, trying to fit every point perfectly, but likely failing to generalize well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b0b601",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Generate synthetic data\n",
    "X = np.linspace(0, 1, 10).reshape(-1, 1)  # 10 random points in range [0, 1]\n",
    "y = 2 * X + 0.3 * np.random.randn(10, 1)  # Linear relationship with noise\n",
    "\n",
    "# Create polynomial features\n",
    "poly = PolynomialFeatures(degree=20)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Fit linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "# Generate predictions\n",
    "X_test = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "y_pred = model.predict(X_test_poly)\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(X, y, color='blue', label='Data')\n",
    "plt.plot(X_test, y_pred, color='red', label='Polynomial Regression (degree=10)')\n",
    "plt.title('Overfitting Example')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.xlim(X.min(), X.max())\n",
    "plt.ylim(y.min(), y.max())\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8cb7b4",
   "metadata": {},
   "source": [
    "## Regularization Techniques\n",
    "\n",
    "Regularization techniques are used to prevent overfitting by adding a penalty for large coefficients in the model. This encourages simpler models that generalize better to unseen data.\n",
    "\n",
    "-   **Ridge Regression** (L2 penalty): Shrinks coefficients to prevent overfitting.\n",
    "-   **Lasso Regression** (L1 penalty): Can shrink some coefficients to zero (feature selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9accd2",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Hyperparameters are settings that are not learned from the data but are set before training the model. Tuning these hyperparameters can significantly improve model performance. Common hyperparameters include:\n",
    "\n",
    "-   Regularization strength (e.g., \\`alpha\\` in Ridge and Lasso).\n",
    "-   Degree of polynomial features.\n",
    "-   Number of trees in ensemble methods (e.g., Random Forest).\n",
    "-   Learning rate in gradient boosting methods.\n",
    "-   etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bcef1a",
   "metadata": {},
   "source": [
    "### Grid Search and Random Search\n",
    "\n",
    "Grid search and random search are techniques for hyperparameter tuning:\n",
    "\n",
    "-   **Grid Search**: Exhaustively searches through a specified subset of hyperparameters, evaluating model performance for each combination.\n",
    "-   **Random Search**: Samples a fixed number of hyperparameter combinations from a specified distribution, often more efficient than grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b26892",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameter grid for Ridge regression\n",
    "params = {'alpha': np.logspace(-3, 3, 7)}\n",
    "\n",
    "# Create a Ridge model\n",
    "ridge = Ridge()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(ridge, param_grid=params, cv=5, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters (Grid Search): {grid_search.best_params_['alpha']:.2f}\")\n",
    "print(f\"Best cross-validation score (Grid Search): {grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Perform random search\n",
    "random_search = RandomizedSearchCV(ridge, param_distributions=params, n_iter=7,\n",
    "                                   cv=5, scoring='r2', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters (Random Search): {random_search.best_params_['alpha']:.2f}\")\n",
    "print(f\"Best cross-validation score (Random Search): {random_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef616b5",
   "metadata": {},
   "source": [
    "## Hands-on Exercises\n",
    "\n",
    "-   Generate synthetic data and visualize overfitting and underfitting using polynomial regression.\n",
    "-   Implement regularization techniques (Ridge and Lasso) and compare their performance.\n",
    "-   Perform hyperparameter tuning using grid search for Ridge and Lasso regression models.\n",
    "-   Visualize the performance of the tuned models on the test set."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "ML Workshop",
   "language": "python",
   "name": "ml_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
