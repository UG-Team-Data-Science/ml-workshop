{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c662f3b2",
   "metadata": {},
   "source": [
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "Support Vector Machines (SVMs) are powerful supervised learning models used for classification and regression tasks. They are particularly effective in high-dimensional spaces and are known for their robustness against overfitting, especially in cases where the number of dimensions exceeds the number of samples.\n",
    "\n",
    "SVMs work by finding the optimal hyperplane that separates different classes in the feature space. The key idea is to maximize the margin between the closest points of the classes, known as support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06f01e",
   "metadata": {},
   "source": [
    "## Mathematical Formulation\n",
    "\n",
    "Given a training set $\\{(x_i, y_i)\\}_{i = 1}^{n}$ where $y_i \\in \\{-1, 1\\}$, the SVM aims to find a hyperplane defined by the equation: $$w^T x + b = 0,$$ where $w$ is the weight vector and $b$ is the bias term. The goal is to maximize the margin defined as: $$\\text{Margin} = \\frac{2}{\\|w\\|}.$$ This leads to the following optimization problem: $$\\begin{align*} \\text{minimize} & \\quad \\frac{1}{2} \\|w\\|^2 \\\\ \\text{subject to} & \\quad y_i (w^T x_i + b) \\geq 1, \\quad \\forall i = 1, \\ldots, n. \\end{align*}$$ This is a convex optimization problem that can be solved using techniques like Lagrange multipliers or quadratic programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7ca668",
   "metadata": {},
   "source": [
    "## Linear SVMs\n",
    "\n",
    "Linear SVMs are used when the data is linearly separable. The algorithm finds the hyperplane that best separates the classes by maximizing the margin. The support vectors are the data points that lie closest to the hyperplane and are critical in defining its position. The hyperplane is defined by the equation: $$w^T x + b = 0,$$ where $w$ is the weight vector and $b$ is the bias term. The decision function for a new data point $x$ is given by: $$f(x) = \\text{sign}(w^T x + b).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97c3685",
   "metadata": {},
   "source": [
    "### Example of Linear SVM\n",
    "\n",
    "To illustrate the concept of linear SVMs, we will use the Iris dataset, and keep only two features for visualization purposes. We will train a linear SVM model and visualize the decision boundary along with the support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7404af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import svm\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data[:, 2:4]\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Pipline for scaling and SVM\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scale features\n",
    "    ('svm', svm.SVC(kernel='linear', C=1.0))  # Linear SVM\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Plotting the decision boundary\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "plt.figure(figsize=(10, 6))\n",
    "DecisionBoundaryDisplay.from_estimator(pipeline, X, response_method='predict',\n",
    "                                       cmap=plt.cm.coolwarm, alpha=0.8,\n",
    "                                       grid_resolution=500)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train,\n",
    "            edgecolors='k', marker='o', s=50, label='Training Data')\n",
    "# plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test,\n",
    "#             edgecolors='k', marker='o', s=50, label='Test Data')\n",
    "# Scale the support vectors back to match the feature scaling\n",
    "support_vectors = pipeline.named_steps['svm'].support_vectors_\n",
    "support_vectors_scaled = pipeline.named_steps['scaler'].inverse_transform(support_vectors)\n",
    "plt.scatter(support_vectors_scaled[:, 0],\n",
    "            support_vectors_scaled[:, 1],\n",
    "            facecolors='none', s=100, edgecolors='k', label='Support Vectors')\n",
    "plt.title('Linear SVM Decision Boundary on Iris Dataset')\n",
    "plt.xlabel('Petal Length (cm)')\n",
    "plt.ylabel('Petal Width (cm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"SVM Accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
    "ConfusionMatrixDisplay.from_estimator(pipeline, X_test, y_test, display_labels=data.target_names,\n",
    "                                      cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e15ba2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=5)\n",
    "print(f\"Cross-Validation Score: {np.mean(cv_scores):.2f} Â± {np.std(cv_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b037e99b",
   "metadata": {},
   "source": [
    "## Nonlinear SVMs\n",
    "\n",
    "When the data is not linearly separable, SVMs can still be applied by transforming the input features into a higher-dimensional space where a linear separation is possible. This is achieved using kernel functions, which implicitly map the data into a higher-dimensional space without the need for explicit computation.\n",
    "\n",
    "Mathematically, the optimization problem remains similar, but the decision function is modified to include a kernel function: $$f(x) = \\sum_{i=1}^{n} \\alpha_i y_i K(x_i, x) + b,$$ where $K(x_i, x)$ is the kernel function that computes the inner product in the transformed space, and $\\alpha_i$ are the Lagrange multipliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2483ca7b",
   "metadata": {},
   "source": [
    "### Example of Nonlinear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fff753",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data for non-linear classification\n",
    "from sklearn.datasets import make_moons\n",
    "X_moons, y_moons = make_moons(n_samples=100, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_moons, X_test_moons, y_train_moons, y_test_moons = \\\n",
    "    train_test_split(X_moons, y_moons, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the SVM model with a radial basis function kernel\n",
    "model_nonlinear = svm.SVC(kernel='rbf', gamma='scale', C=1.0)\n",
    "model_nonlinear.fit(X_train_moons, y_train_moons)\n",
    "\n",
    "# Plotting the decision boundary\n",
    "plt.figure(figsize=(10, 6))\n",
    "DecisionBoundaryDisplay.from_estimator(model_nonlinear, X_moons, response_method='predict',\n",
    "                                       cmap=plt.cm.coolwarm, alpha=0.8,\n",
    "                                       grid_resolution=500)\n",
    "# plt.scatter(X_train_moons[:, 0], X_train_moons[:, 1], c=y_train_moons,\n",
    "#             edgecolors='k', marker='o', s=50, label='Training Data')\n",
    "plt.scatter(X_test_moons[:, 0], X_test_moons[:, 1], c=y_test_moons,\n",
    "            edgecolors='k', marker='o', s=50, label='Test Data')\n",
    "plt.title('Nonlinear SVM Decision Boundary on Moons Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8026504f",
   "metadata": {},
   "source": [
    "## Kernel Trick\n",
    "\n",
    "SVMs can efficiently handle non-linear decision boundaries using the kernel trick. Instead of explicitly mapping the data into a higher-dimensional space, SVMs use kernel functions to compute the inner products in that space. Common kernel functions include:\n",
    "\n",
    "-   **Linear Kernel**: $K(x_i, x_j) = x_i^T x_j$\n",
    "-   **Polynomial Kernel**: $K(x_i, x_j) = (x_i^T x_j + c)^d$\n",
    "-   **Radial Basis Function (RBF) Kernel**: $K(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right)$\n",
    "-   **Sigmoid Kernel**: $K(x_i, x_j) = \\tanh(\\alpha x_i^T x_j + c)$\n",
    "\n",
    "The choice of kernel allows SVMs to adapt to various data distributions and complexities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96ee10d",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages\n",
    "\n",
    "-   **Advantages**:\n",
    "    -   Effective in high-dimensional spaces.\n",
    "    -   Robust against overfitting, especially in high-dimensional datasets.\n",
    "    -   Works well with clear margin of separation.\n",
    "    -   Can handle non-linear decision boundaries using kernel functions.\n",
    "-   **Disadvantages**:\n",
    "    -   Computationally intensive, especially with large datasets.\n",
    "    -   Requires careful tuning of hyperparameters (e.g., regularization parameter $C$ and kernel parameters).\n",
    "    -   Less effective on noisy datasets with overlapping classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e2e795",
   "metadata": {},
   "source": [
    "## SVMs as Regression Models\n",
    "\n",
    "SVMs can also be used for regression tasks, known as Support Vector Regression (SVR). The main idea is similar to classification, where the goal is to find a function that approximates the relationship between input features and continuous output values. The optimization problem for SVR is formulated to minimize the error while maintaining a margin of tolerance around the predicted values. The SVR can also utilize kernel functions to handle non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9445b968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "# Generate synthetic data for regression\n",
    "X_reg = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y_reg = np.sin(X_reg).ravel() + np.random.normal(0, 0.1, X_reg.shape[0])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = \\\n",
    "    train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline for scaling and SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pipeline_reg = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scale features\n",
    "    ('svr', SVR(kernel='rbf', C=1.0, epsilon=0.1))  # SVR with RBF kernel\n",
    "])\n",
    "# Fit the SVR model\n",
    "pipeline_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_reg = pipeline_reg.predict(X_test_reg)\n",
    "\n",
    "# Evaluate the SVR model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "print(f\"SVR Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"SVR R^2 Score: {r2:.2f}\")\n",
    "\n",
    "# Plotting the SVR results\n",
    "plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(X_train_reg, y_train_reg, color='blue', label='Training Data')\n",
    "plt.scatter(X_test_reg, y_test_reg, color='green', label='Test Data')\n",
    "plt.scatter(X_test_reg, y_pred_reg, color='red', label='SVR Predictions')\n",
    "plt.plot(np.sort(X_test_reg, axis=0), pipeline_reg.predict(np.sort(X_test_reg, axis=0)),\n",
    "         color='orange', label='SVR Model')\n",
    "plt.title('Support Vector Regression on Synthetic Data')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d856b",
   "metadata": {},
   "source": [
    "This code demonstrates how to use SVM for regression tasks, showing the model's predictions against the actual data points. The SVR is configured with a radial basis function kernel, allowing it to capture non-linear relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aee470",
   "metadata": {},
   "source": [
    "## Hands-on Exercises\n",
    "\n",
    "-   **Experiment with Different Kernels**: Using the moons dataset, modify the SVM model to use different kernel functions (e.g., polynomial, sigmoid) and observe how the decision boundary changes. Compare the performance of each kernel using accuracy and confusion matrix.\n",
    "-   **Hyperparameter Tuning**: Use techniques like grid search or random search to find the optimal hyperparameters for the SVM model, such as the regularization parameter $C$ and kernel parameters. Evaluate the impact of these hyperparameters on model performance.\n",
    "-   **Visualizing Decision Boundaries**: Create visualizations of the decision boundaries for different SVM models on the moons dataset. Use contour plots to illustrate how the decision boundaries change with different hyperparameters and kernels.\n",
    "-   **Support Vector Regression**: Implement a regression task using SVR on the following synthetic dataset: $$y = x^2 + \\cos(2\\pi x) + \\epsilon,$$ where $\\epsilon$ is Gaussian noise. Experiment with different kernel functions and hyperparameters, and evaluate the model's performance using metrics like mean squared error and $R^2$ score. Visualize the predictions against the actual data points to understand how well the SVR captures the underlying relationship."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "ML Workshop",
   "language": "python",
   "name": "ml_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
