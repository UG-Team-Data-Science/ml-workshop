{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eecd07c",
   "metadata": {},
   "source": [
    "\n",
    "# The Machine Learning Workflow\n",
    "\n",
    "A typical machine learning workflow is a structured process that guides the development of machine learning models. It consists of several key steps, each contributing to the successful deployment of a model. The workflow can be summarized as follows:\n",
    "\n",
    "1.  ****Problem Definition****: Clearly define the problem you want to solve with machine learning. This includes understanding the context and the specific goals of the project.\n",
    "2.  ****Data Collection****: Gather the necessary data that will be used to train and evaluate the model. This data can come from various sources, such as databases, APIs, or web scraping.\n",
    "3.  ****Data Exploration and Preprocessing****: Analyze the collected data to understand its structure, quality, and characteristics. This step often involves cleaning the data, handling missing values, and transforming features to make them suitable for modeling.\n",
    "4.  ****Model Selection****: Choose the appropriate machine learning algorithms based on the problem type (e.g., classification, regression) and the nature of the data. This may involve experimenting with different models to find the best fit.\n",
    "5.  ****Model Training and Validation****: Train the selected model on the training dataset and validate its performance using a separate validation set. This step includes tuning hyperparameters to optimize the model's performance.\n",
    "6.  ****Model Evaluation****: Assess the model's performance using metrics relevant to the problem, such as accuracy, precision, recall, or F1 score. This evaluation helps determine if the model meets the desired criteria.\n",
    "7.  ****Model Deployment and Monitoring****: Once the model is deemed satisfactory, deploy it to a production environment where it can make predictions on new data. Continuous monitoring is essential to ensure the model remains effective over time, especially as new data becomes available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c417059",
   "metadata": {},
   "source": [
    "## Practical Demonstration\n",
    "\n",
    "We will demonstrate a simple machine learning workflow using the California housing dataset. This dataset is commonly used for regression tasks and contains information about various features of houses in California, along with their median house values.\n",
    "\n",
    "We begin by loading the dataset, exploring its structure, and then proceeding through the steps of the machine learning workflow, including data preprocessing, model training, and evaluation (we will not demonstrate deployment in this notebook).\n",
    "\n",
    "-   ****Load the California housing dataset****: This dataset is available in the `sklearn.datasets` module and can be easily loaded into a pandas `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71da8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec67de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5005e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some key information about the dataset\n",
    "print(\"California Housing Dataset\")\n",
    "print(data.DESCR)  # Description of the dataset\n",
    "print(\"Feature names:\", data.feature_names)\n",
    "print(\"Target variable:\", data.target.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14c0100",
   "metadata": {},
   "source": [
    "-   ****Quick exploration****: We will check the shape of the dataset, the names of the columns, and identify the target variable; we will also visualize the correlation matrix to understand the relationships between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e91f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"Target:\", data.target.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940fc859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(df.corr(), annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
    "plt.title('Correlation Matrix of California Housing Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48cb8a0",
   "metadata": {},
   "source": [
    "-   ****Split features and target****: We will separate the features (independent variables) from the target variable (dependent variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91afe480",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[data.target.name])\n",
    "y = df[data.target.name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbd4290",
   "metadata": {},
   "source": [
    "-   ****Train-test split****: We will split the dataset into training and testing sets to evaluate the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb0674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed604b0",
   "metadata": {},
   "source": [
    "-   ****Model training****: We will use a simple linear regression model to fit the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b97f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae83132",
   "metadata": {},
   "source": [
    "-   ****Prediction and evaluation****: Finally, we will make predictions on the test set and evaluate the model's performance using mean squared error (MSE) as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"MSE on test set: {mse:.2f}\")\n",
    "\n",
    "r2 = model.score(X_test, y_test)\n",
    "print(f\"R-squared on test set: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd82f35",
   "metadata": {},
   "source": [
    "We can also visualize the predictions against the actual values to get a better understanding of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06817324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.regplot(x=y_test, y=y_pred, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8ddef8",
   "metadata": {},
   "source": [
    "Finally, let's save the California housing dataset to a CSV file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb0896",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('../../data/california_housing.csv', index=False)\n",
    "print(\"California housing dataset saved to 'california_housing.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c6a2f",
   "metadata": {},
   "source": [
    "## Hands-on Exercises\n",
    "\n",
    "Use the Ames housing dataset from OpenML to practice the machine learning workflow. The Ames housing dataset is a well-known dataset for regression tasks, containing various features of houses in Ames, Iowa, along with their sale prices. The dataset is available on OpenML and can be loaded using the `fetch_openml` function from `sklearn.datasets`.\n",
    "\n",
    "-   Load the Ames housing dataset from OpenML (`house_prices`);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331c0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "\n",
    "data = fetch_openml('house_prices', as_frame=True, parser=\"auto\")\n",
    "df = data.frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba0dec3",
   "metadata": {},
   "source": [
    "-   Explore its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ames Housing Dataset\")\n",
    "print(\"Number of samples:\", df.shape[0])\n",
    "print(\"Number of features:\", df.shape[1] - 1)\n",
    "print(\"Target variable:\", data.target.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef4010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe46e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select the numeric columns for correlation analysis\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "plt.figure(figsize=(30, 24))\n",
    "sns.heatmap(df[numeric_cols].corr(), annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
    "plt.title('Correlation Matrix of Ames Housing Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1c7065",
   "metadata": {},
   "source": [
    "-   Retain only the feature with the highest correlation to the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27c20ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain only a single feature for simplicity\n",
    "df = df[['OverallQual', 'SalePrice']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83009fe9",
   "metadata": {},
   "source": [
    "-   Split the features and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9bca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=[data.target.name])\n",
    "y = df[data.target.name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c3069e",
   "metadata": {},
   "source": [
    "-   Perform the train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fc939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd1975a",
   "metadata": {},
   "source": [
    "-   Choose a different regression model, such as Decision Tree Regression or Random Forest Regression (you should search on the web for how to use these models in `scikit-learn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2871671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Random Forest Regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f21befa",
   "metadata": {},
   "source": [
    "-   Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1cad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4256254a",
   "metadata": {},
   "source": [
    "-   Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d9952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"MSE on test set: {mse:.2f}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared on test set: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad2a3c8",
   "metadata": {},
   "source": [
    "-   Visualize the predictions against the actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f67fc9c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.regplot(x=y_test, y=y_pred, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db084e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, we have covered the machine learning workflow, including problem definition, data collection, exploration, preprocessing, model selection, training, evaluation, and deployment. We demonstrated a simple workflow using the California housing dataset, and we practiced the workflow using the Ames housing dataset. The key steps in the workflow are essential for building effective machine learning models, and understanding these steps will help you in your future machine learning projects."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "ML Workshop",
   "language": "python",
   "name": "ml_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
