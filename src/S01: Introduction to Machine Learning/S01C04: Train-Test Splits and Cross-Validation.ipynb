{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee1d9352",
   "metadata": {},
   "source": [
    "\n",
    "# Train-Test Splits and Cross-Validation\n",
    "\n",
    "Training a machine learning model involves using data to learn patterns and make predictions. There is, however, a risk to training a model too well on some particular dataset, which is that the model may not generalize well to new, unseen data. To mitigate this risk, we use techniques like train-test splits and cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4929bfa",
   "metadata": {},
   "source": [
    "## Train-Test Splits\n",
    "\n",
    "A train-test split divides the dataset into two parts:\n",
    "\n",
    "-   ****Training Set****: Used to fit the model.\n",
    "-   ****Test Set****: Used to evaluate the model's performance on unseen data.\n",
    "\n",
    "The split is usually done before any preprocessing of the data to avoid data leakage from the test set into the training set.\n",
    "\n",
    "The split ratio can vary, but common practices include:\n",
    "\n",
    "-   ****80/20 Split****: 80% training, 20% testing.\n",
    "-   ****70/30 Split****: 70% training, 30% testing.\n",
    "-   ****90/10 Split****: 90% training, 10% testing.\n",
    "\n",
    "This ensures that the model is trained on a substantial amount of data while still having enough data to evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6a8344",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "Each machine learning model usually depends on a set of hyperparameters that need to be manually tuned to achieve the best performance. Cross-validation (CV) is a technique that helps to tune these hyperparameters and assess the model's performance more robustly.\n",
    "\n",
    "Cross-validation involves splitting the training dataset into multiple subsets (folds) and training the model multiple times:\n",
    "\n",
    "-   ****$K$-Fold Cross-Validation****: The dataset is divided into $K$ subsets (folds). The model is trained $K$ times, each time using $K - 1$ folds for training and 1 fold for validation. The performance is averaged over all $K$ iterations.\n",
    "-   ****Leave-One-Out Cross-Validation (LOOCV)****: A special case of $K$-fold where $K$ is equal to the number of samples in the dataset. Each sample is used once as a validation set while the rest are used for training. This is computationally expensive but can be useful for small datasets.\n",
    "-   ****Stratified Cross-Validation****: Ensures that each fold has the same proportion of classes as the entire dataset. This is particularly useful for imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec65fbb1",
   "metadata": {},
   "source": [
    "## Practical Demonstration\n",
    "\n",
    "We will demonstrate the use of train-test splits and cross-validation using the California housing dataset, which is a regression dataset available in the `sklearn.datasets` module.\n",
    "\n",
    "We will go through the Machine Learning workflow steps, but skip some of the steps that are not relevant for this demonstration.\n",
    "\n",
    "-   Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb2e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6d468",
   "metadata": {},
   "source": [
    "-   Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60bd096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Plot the correlation matrix of the features and the target variable\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix of California Housing Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b818f7e",
   "metadata": {},
   "source": [
    "Since the only meaningful correlation with the target variable (house value) is with the `MedInc` (median income) feature, we will keep only this feature for our demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56d675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the 'MedInc' feature for simplicity\n",
    "X = df[['MedInc']]\n",
    "y = df['MedHouseVal']\n",
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82406cb0",
   "metadata": {},
   "source": [
    "-   Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eadb4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Simple train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f5133",
   "metadata": {},
   "source": [
    "This code splits the dataset into training and testing sets, with 20% of the data reserved for testing. The `random_state` parameter ensures that the split is reproducible.\n",
    "\n",
    "Let's now visualize the distribution of the target variable in the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4a88e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(y_train, bins=30, color='blue', alpha=0.7, label='Train Set')\n",
    "plt.title('Training Set Target Distribution')\n",
    "plt.xlabel('Median House Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(y_test, bins=30, color='orange', alpha=0.7, label='Test Set')\n",
    "plt.title('Test Set Target Distribution')\n",
    "plt.xlabel('Median House Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434eac7c",
   "metadata": {},
   "source": [
    "This visualization shows the distribution of house values in both the training and test sets, indicating that the split has preserved the overall distribution of the target variable.\n",
    "\n",
    "-   Model training and evaluation:\n",
    "\n",
    "We begin by training a simple linear regression model on the reduced training set and then evaluating its performance on the test set using the $R^2$ metric, which indicates how well the model explains the variance in the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a01e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "score_test = model.score(X_test, y_test)\n",
    "print(f\"Test set R²: {score_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed05fe",
   "metadata": {},
   "source": [
    "We can try and improve the model evaluation by using cross-validation, which will give us a better estimate of the model's performance by averaging the results over multiple train-test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37325ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "model = LinearRegression()\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring=\"r2\")\n",
    "print(f\"Cross-validation R² scores: {cv_scores}\")\n",
    "print(f\"CV R²: {np.mean(cv_scores):.2f} +/- {np.std(cv_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4b15e0",
   "metadata": {},
   "source": [
    "This code performs 5-fold cross-validation on the entire dataset, providing a more robust estimate of the model's performance. The `cross_val_score` function automatically handles the train-test splits internally, allowing us to focus on the model evaluation.\n",
    "\n",
    "We can use the `cross_val_predict` function to get the predicted values for each fold, which can be useful for further analysis or visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Get cross-validated predictions\n",
    "y_pred = cross_val_predict(model, X_train, y_train, cv=5)\n",
    "print(f\"Cross-validated predictions: {y_pred[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aea7b1",
   "metadata": {},
   "source": [
    "There was no real improvement in the model's performance, but this is expected since we are using a very simple model with only one feature. In practice, cross-validation is particularly useful when dealing with more complex models and larger datasets, as it helps to ensure that the model generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8d59a5",
   "metadata": {},
   "source": [
    "## Hands-on Exercises\n",
    "\n",
    "Using the Ames Housing dataset:\n",
    "\n",
    "-   Load the dataset\n",
    "-   Explore the dataset and select a few relevant features (e.g., `GrLivArea`, `OverallQual`, `YearBuilt`, `TotalBsmtSF`).\n",
    "-   Do a standard 80/20 train-test split.\n",
    "-   Train a `LinearRegression` model and compute $R^2$ on the test set.\n",
    "-   Use 5-fold cross-validation (`cross_val_score`) on the entire dataset and compare the average $R^2$.\n",
    "-   Optional: Try changing the `cv` value (e.g., 10) and see how scores vary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f6338",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, we learned about the importance of train-test splits and cross-validation in machine learning. We demonstrated how to perform a train-test split, train a model, and evaluate its performance using the California housing dataset. We also explored the use of cross-validation to obtain a more robust estimate of model performance."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "ML Workshop",
   "language": "python",
   "name": "ml_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
