{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d6f6b6",
   "metadata": {},
   "source": [
    "\n",
    "# Data Preprocessing\n",
    "\n",
    "A key step in any machine learning project is data preprocessing, which prepares raw data for model training and evaluation. Proper preprocessing can significantly improve model performance and robustness, while poor preprocessing can lead to misleading results or even model failure.\n",
    "\n",
    "A few common preprocessing tasks include:\n",
    "\n",
    "1.  ****Handling missing values****: Missing data can skew results or cause algorithms to fail. Common strategies of handling missing values include:\n",
    "    -   Removing rows/columns with missing values\n",
    "    -   Imputing missing values using mean, median, or mode\n",
    "    -   Using advanced techniques like KNN imputation\n",
    "2.  ****Feature scaling****: Many algorithms are sensitive to the scale of input features. Common methods include:\n",
    "    -   Standardization (z-score normalization): Rescales features to have mean 0 and standard deviation 1\n",
    "    -   Min-Max scaling: Rescales features to a fixed range, typically [0, 1]\n",
    "3.  ****Encoding categorical variables****: Machine learning algorithms typically require numerical input. Common techniques include:\n",
    "    -   One-hot encoding: Converts categorical variables into binary columns\n",
    "    -   Label encoding: Assigns a unique integer to each category\n",
    "4.  ****Removing duplicates****: Duplicate rows can bias model training and evaluation.\n",
    "5.  ****Feature engineering****: Creating new features from existing ones can improve model performance. Examples include:\n",
    "    -   Polynomial features\n",
    "    -   Interaction terms\n",
    "6.  ****Pipeline creation****: Using pipelines to chain preprocessing steps and model fitting can simplify the workflow and ensure consistent preprocessing across training and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d6f368",
   "metadata": {},
   "source": [
    "## Practical Demonstration\n",
    "\n",
    "We will demonstrate data preprocessing using the `pandas` library for data manipulation and `scikit-learn` for machine learning tasks. The dataset we will use is the Titanic dataset, which contains information about passengers on the Titanic and whether they survived."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee55a0",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "-   Loading the Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab39d4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load the Titanic dataset from scikit-learn\n",
    "data = fetch_openml('titanic', version=1, as_frame=True)\n",
    "print(data['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c04a8d",
   "metadata": {},
   "source": [
    "-   Convert the dataset to a pandas DataFrame and display the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa664b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df = data.frame\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afca283",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "We'll start by doing a bit of EDA to understand the dataset better. We begin by visualising the distribution of the target variable `survived`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69109ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.countplot(x='survived', data=df)\n",
    "plt.title('Survival Distribution')\n",
    "plt.xlabel('Survived')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b354a5",
   "metadata": {},
   "source": [
    "As a first guess, we might hypothesize that the survival rate is higher might depend on the socio-economic status, which we can infer from the `fare` column. Let's visualize the distribution of fare for survivors and non-survivors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6fcd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='survived', y='fare', data=df, log_scale=True)\n",
    "plt.title('Fare Distribution by Survival')\n",
    "plt.xlabel('Survived')\n",
    "plt.ylabel('Fare')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258a5fb",
   "metadata": {},
   "source": [
    "This seems to be borne out by the data, as we can see that survivors tend to have higher fares than non-survivors. Another indication of socio-economic status is the passenger class, which we can visualize as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d9a044",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='pclass', hue='survived', data=df)\n",
    "plt.title('Passenger Class vs Survival')\n",
    "plt.xlabel('Passenger Class')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Survived', loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1e72d",
   "metadata": {},
   "source": [
    "Another hypothesis we might have is that the survival rate might depend on the gender of the passengers, e.g. \"Women and children first\". Let's visualize the distribution of survival by gender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='sex', hue='survived', data=df)\n",
    "plt.title('Gender vs Survival')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Survived', loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2241d",
   "metadata": {},
   "source": [
    "We will only keep the features mentioned in the hypotheses above, which are `fare`, and `sex`. We will also keep the target variable `survived` for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80599e9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df = df[['fare', 'sex', 'survived']]\n",
    "X = df.drop(columns=data.target.name)\n",
    "y = df[data.target.name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad7743f",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "Perform a train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8648eed9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6324afd",
   "metadata": {},
   "source": [
    "### Data Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8baea23",
   "metadata": {},
   "source": [
    "#### Handling Missing Values\n",
    "\n",
    "Missing values are common in real-world datasets. In the Titanic dataset, the `fare` column has some missing values. We will handle these missing values by imputing them with the mean fare.\n",
    "\n",
    "First, let's check for missing values in the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f678cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.isnull().sum())\n",
    "print(X_test.isnull().sum())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cfa82a",
   "metadata": {},
   "source": [
    "We can see that the `fare` column has some (one) missing values. We will fill these missing values with the mean fare using the `SimpleImputer` class from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed8e5d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Fill missing values in the 'fare' column with the mean\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train[['fare']] = imputer.fit_transform(X_train[['fare']])\n",
    "X_test[['fare']] = imputer.transform(X_test[['fare']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab681a",
   "metadata": {},
   "source": [
    "#### Feature Scaling\n",
    "\n",
    "Feature scaling is important for algorithms that are sensitive to the scale of input features, such as logistic regression, k-nearest neighbors, and support vector machines. We will scale the `fare` feature using standardization (z-score normalization), which rescales the feature to have mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88e0499",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train[['fare']] = scaler.fit_transform(X_train[['fare']])\n",
    "\n",
    "# Transform the test data\n",
    "X_test[['fare']] = scaler.transform(X_test[['fare']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420d67a0",
   "metadata": {},
   "source": [
    "#### Encoding Categorical Variables\n",
    "\n",
    "Categorical variables need to be converted into numerical format for machine learning algorithms. Depending on the type of categorical variable, i.e. whether it is ordinal or nominal, we can use different encoding techniques. For a nominal categorical variable, for which the categories do not have a natural order, we can use **one-hot encoding**. For an ordinal categorical variable, where the categories have a natural order, we can use **label encoding**.\n",
    "\n",
    "In the reduced Titanic dataset, the only categorical variable is `sex`, which is a nominal variable with two categories, `male` and `female`. We will use one-hot encoding to convert this categorical variable into numerical format. One-hot encoding creates binary columns for each category, indicating the presence or absence of that category in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5671f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2615185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create a OneHotEncoder instance\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "\n",
    "# Encode the 'sex' column\n",
    "X_train_encoded = one_hot_encoder.fit_transform(X_train[['sex']]).toarray()\n",
    "X_test_encoded = one_hot_encoder.transform(X_test[['sex']]).toarray()\n",
    "# Create a DataFrame with the encoded columns\n",
    "import pandas as pd\n",
    "X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=one_hot_encoder.get_feature_names_out(['sex']))\n",
    "X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=one_hot_encoder.get_feature_names_out(['sex']))\n",
    "# Concatenate the encoded columns with the original DataFrame\n",
    "X_train = pd.concat([X_train.reset_index(drop=True), X_train_encoded_df], axis=1)\n",
    "X_test = pd.concat([X_test.reset_index(drop=True), X_test_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original 'sex' column\n",
    "X_train = X_train.drop(columns=['sex'])\n",
    "X_test = X_test.drop(columns=['sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c85c70d",
   "metadata": {},
   "source": [
    "We have now preprocessed the dataset, and it is ready for training a machine learning model. Here's how the preprocessed training data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c9917",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa8030",
   "metadata": {},
   "source": [
    "We've only kept two features: `fare` (numerical) and `sex` (categorical). The target variable is `survived`, which we want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5a7886",
   "metadata": {},
   "source": [
    "### Training a Machine Learning Model\n",
    "\n",
    "Now that we have preprocessed the data, we can train a machine learning model. We are dealing with a binary classification problem (predicting survival), so we will use logistic regression as our model. Logistic regression is a simple yet effective algorithm for binary classification tasks, and we will discuss it in more detail in a later session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1813a9b7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b3e030",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "After training the model, we need to evaluate its performance on the test data. We will use the accuracy metric, which measures the proportion of correct predictions made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f9d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f'Model accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf9039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['did not survive', 'survived']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823805a3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Plot the normalized confusion matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=['Not Survived', 'Survived'],\n",
    "            yticklabels=['Not Survived', 'Survived'])\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe845e4",
   "metadata": {},
   "source": [
    "## Hands-on Exercises\n",
    "\n",
    "In this hands-on exercise, you will apply the data preprocessing to the Iris dataset, with a twist: instead of considering the species as the target variable, you will consider the petal length as the target variable. The ultimate goal is to predict the petal length based on the other features (sepal length, sepal width, petal width, and species).\n",
    "\n",
    "-   Load the Iris dataset\n",
    "-   Extract the features and target variable\n",
    "    -   Features: sepal length, sepal width, petal width, species\n",
    "    -   Target variable: petal length\n",
    "-   Perform a train-test split\n",
    "-   Preprocess the data:\n",
    "    -   Handle missing values (if any)\n",
    "    -   Scale the numerical features (sepal length, sepal width, petal width)\n",
    "    -   Encode the categorical variable (species) using one-hot encoding\n",
    "    -   Remove any unnecessary columns\n",
    "-   Train a linear regression model to predict the petal length using the preprocessed data"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "ML Workshop",
   "language": "python",
   "name": "ml_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
