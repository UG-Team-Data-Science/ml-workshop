{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96537c8e",
   "metadata": {},
   "source": [
    "\n",
    "# Regularized Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c6fb77",
   "metadata": {},
   "source": [
    "## How do Machine Learning Models Learn?\n",
    "\n",
    "Machine learning models learn from data by identifying patterns and relationships between the independent variables (features) and the dependent variable (target). The learning process involves adjusting the model parameters to minimize the difference between the predicted values and the actual values of the target variable. This is typically done by minimizing a cost function, which quantifies the error in the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c749acc8",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "The cost function, measures how well the model's predictions match the actual target values. In regression tasks, a common cost function is the Mean Squared Error (MSE), which is defined as: $$J(w) = \\frac{1}{2m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2$$ where:\n",
    "\n",
    "-   $J(w)$ is the cost function,\n",
    "-   $m$ is the number of samples,\n",
    "-   $y_i$ is the actual target value for the $i$-th sample,\n",
    "-   $\\hat{y}_i$ is the predicted target value for the $i$-th sample,\n",
    "-   $w$ represents the model parameters (coefficients)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c6242",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function by iteratively updating the model parameters in the direction of the steepest descent. The update rule for gradient descent is given by: $$w := w - \\alpha \\frac{\\partial}{\\partial w}J(w)$$ where:\n",
    "\n",
    "-   $w$ is the vector of model parameters,\n",
    "-   $\\alpha$ is the learning rate (a hyperparameter that controls the step size of each update),\n",
    "-   $\\displaystyle\\frac{\\partial}{\\partial w}J(w)$ is the gradient of the cost function with respect to the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0b4cd",
   "metadata": {},
   "source": [
    "### Gradient Descent Algorithm\n",
    "\n",
    "The gradient descent algorithm can be summarized in the following steps:\n",
    "\n",
    "-   **Initialize Parameters**: Start with random values for the model parameters (coefficients).\n",
    "-   **Compute Predictions**: Use the current parameters to compute the predicted values for the training data.\n",
    "-   **Calculate Cost**: Compute the cost function (e.g., MSE) using the predicted values and the actual target values.\n",
    "-   **Compute Gradient**: Calculate the gradient of the cost function with respect to the model parameters.\n",
    "-   **Update Parameters**: Update the model parameters using the gradient descent update rule.\n",
    "-   **Repeat**: Repeat until convergence (i.e., when the change in the cost function is below a certain threshold or a maximum number of iterations is reached)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b370a9f8",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting\n",
    "\n",
    "There are a few caveats to consider when working with machine learning models in general, and regression in particular. These include overfitting and underfitting, which can significantly affect the performance of your model.\n",
    "\n",
    "-   ****Overfitting****: occurs when a model learns the noise in the training data rather than the underlying pattern. This leads to a model that performs well on the training data but poorly on unseen data.\n",
    "-   ****Underfitting****: occurs when a model is too simple to capture the underlying pattern in the data. This results in poor performance on both the training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb1ab32",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "To address overfitting and underfitting, we can use regularization techniques. Regularization helps to constrain the model complexity by adding a penalty term to the loss function. This penalty discourages the model from fitting the noise in the data, leading to better generalization on unseen data. Regularization is particularly useful when dealing with high-dimensional datasets or when the number of features is greater than the number of observations.\n",
    "\n",
    "Some common regularization techniques include:\n",
    "\n",
    "1.  **Lasso Regression (L1 Regularization)**: Adds a penalty equal to the absolute value of the magnitude of coefficients. This can lead to sparse models where some coefficients are exactly zero, effectively performing feature selection.$$J(w) = \\frac{1}{2m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} |w_j|$$ where $\\lambda$ is the regularization strength and $p$ is the number of features.\n",
    "2.  **Ridge Regression (L2 Regularization)**: Adds a penalty equal to the square of the magnitude of coefficients. This helps to shrink the coefficients but does not lead to sparse models. $$J(w) = \\frac{1}{2m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} w_j^2$$ where $\\lambda$ is the regularization strength and $p$ is the number of features.\n",
    "3.  ****Elastic Net****: Combines both L1 and L2 regularization, allowing for a balance between feature selection and coefficient shrinkage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2685be7",
   "metadata": {},
   "source": [
    "## Practical Demonstration\n",
    "\n",
    "We will now demonstrate how to implement regularized regression using Lasso, Ridge, and Elastic Net regression methods from the `scikit-learn` library. We will use a synthetic dataset for this demonstration, but you can apply the same techniques to any regression dataset. In particular, we will introduce some correlations between features, which will allow us to see how regularization can help in feature selection and model performance.\n",
    "\n",
    "-   Creating a Synthetic Dataset with Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a234d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "n_samples, n_features = 100, 1\n",
    "\n",
    "X, y, coef = make_regression(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    noise=7.5,\n",
    "    coef=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(X, columns=['feature'])\n",
    "df['target'] = y\n",
    "\n",
    "print(f\"Original coefficients: {coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ab9f4e",
   "metadata": {},
   "source": [
    "-   Plotting the Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e5cc1-6964-4a88-b3e2-0db60ff13604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='feature', y='target', color='blue', label='Data Points')\n",
    "plt.title('Synthetic Dataset')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655a3eb8",
   "metadata": {},
   "source": [
    "-   Introduce some correlation between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f4866",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack((X, 2 * X[:, 0] + 0.01 * np.random.randn(n_samples)))  # Add a correlated feature\n",
    "y += 0.5 * coef * X[:, 1]  # Introduce correlation with the target variable\n",
    "\n",
    "# Adjust coefficients for the new feature\n",
    "true_coefs = np.array([coef, 0.5 * coef])\n",
    "print(f\"Correlated coefficients: {true_coefs}\")\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "df['target'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854080f9",
   "metadata": {},
   "source": [
    "-   Train a Simple Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6354788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a simple linear regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Linear Regression R^2 Score: {linear_model.score(X_test, y_test):.4f}')\n",
    "print(f'Linear Regression Coefficients: {linear_model.coef_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d23e0d9",
   "metadata": {},
   "source": [
    "-   Train a Lasso Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c01151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Train a Lasso regression model with alpha=0.1\n",
    "lasso_model = Lasso(alpha=0.01)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Lasso Regression R^2 Score: {lasso_model.score(X_test, y_test):.4f}')\n",
    "print(f'Lasso Regression Coefficients: {lasso_model.coef_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423bc24d",
   "metadata": {},
   "source": [
    "-   Train a Ridge Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837311a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Train a Ridge regression model with alpha=0.1\n",
    "ridge_model = Ridge(alpha=0.1)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Ridge Regression R^2 Score: {ridge_model.score(X_test, y_test):.4f}')\n",
    "print(f'Ridge Regression Coefficients: {ridge_model.coef_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f94eb7",
   "metadata": {},
   "source": [
    "-   Train an Elastic Net Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd72a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Train an Elastic Net regression model with alpha=0.1 and l1_ratio=0.5\n",
    "elastic_net_model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_net_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Elastic Net Regression R^2 Score: {elastic_net_model.score(X_test, y_test):.4f}')\n",
    "print(f'Elastic Net Regression Coefficients: {elastic_net_model.coef_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a607795",
   "metadata": {},
   "source": [
    "-   Evaluate all the Models and visualize their predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234efd5b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Predictions from all models\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "y_pred_elastic_net = elastic_net_model.predict(X_test)\n",
    "\n",
    "# Calculate R^2 scores\n",
    "r2_scores = {\n",
    "    'Linear Regression': linear_model.score(X_test, y_test),\n",
    "    'Lasso Regression': lasso_model.score(X_test, y_test),\n",
    "    'Ridge Regression': ridge_model.score(X_test, y_test),\n",
    "    'Elastic Net Regression': elastic_net_model.score(X_test, y_test)\n",
    "}\n",
    "\n",
    "print(\"R^2 Scores:\")\n",
    "for model, score in r2_scores.items():\n",
    "    print(f\"{model}: {score:.4f}\")\n",
    "\n",
    "# Visualize predictions\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(y_test, y_pred_linear, label='Linear Regression', alpha=0.5)\n",
    "plt.scatter(y_test, y_pred_lasso, label='Lasso Regression', alpha=0.5)\n",
    "plt.scatter(y_test, y_pred_ridge, label='Ridge Regression', alpha=0.5)\n",
    "plt.scatter(y_test, y_pred_elastic_net, label='Elastic Net Regression', alpha=0.5)\n",
    "\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')\n",
    "plt.title('Model Predictions vs Actual Values')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade0907",
   "metadata": {},
   "source": [
    "## Hands-on Exercises\n",
    "\n",
    "-   **Lasso Regression**: Implement Lasso regression on a dataset of your choice. Experiment with different values of the regularization parameter `alpha` until you find the best model.\n",
    "-   **Ridge Regression**: Implement Ridge regression on the same dataset. Again, experiment with different values of `alpha` and compare the results with Lasso regression.\n",
    "-   **Feature Importance**: Analyze the feature importance in both Lasso and Ridge regression models."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "ML Workshop",
   "language": "python",
   "name": "ml_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
