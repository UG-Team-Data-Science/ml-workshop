{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79c77d6",
   "metadata": {},
   "source": [
    "\n",
    "# Dimensionality Reduction Techniques\n",
    "\n",
    "**Dimensionality reduction** is a crucial step in data preprocessing, especially when dealing with high-dimensional datasets. It aims to reduce the number of features while preserving the essential structure and relationships in the data. This can help improve the performance of machine learning models, reduce computational costs, and facilitate data visualization. Common techniques for dimensionality reduction include:\n",
    "\n",
    "-   **Principal Component Analysis (PCA)**: A linear technique that transforms the data into a new coordinate system where the greatest variance lies along the first axis (principal component), the second greatest variance along the second axis, and so on.\n",
    "-   **Autoencoders**: Neural networks that learn to encode the input data into a lower-dimensional representation and then decode it back to the original space. They are particularly useful for learning complex, non-linear mappings and can be used for both dimensionality reduction and feature extraction.\n",
    "-   **Linear Discriminant Analysis (LDA)**: A supervised technique that projects the data onto a lower-dimensional space while maximizing the separation between classes. It is particularly useful when the goal is to improve classification performance by reducing dimensionality.\n",
    "-   **Independent Component Analysis (ICA)**: A technique that separates a multivariate signal into additive independent components. It is often used in signal processing and blind source separation tasks, such as separating mixed signals in audio or image processing.\n",
    "-   **t-Distributed Stochastic Neighbor Embedding (t-SNE)**: A non-linear technique that visualizes high-dimensional data by minimizing the divergence between probability distributions of pairwise similarities in high and low dimensions. It is particularly effective for visualizing clusters in high-dimensional data.\n",
    "-   **Feature Selection**: While not a dimensionality reduction technique in the strictest sense, feature selection involves selecting a subset of relevant features from the original dataset. This can be done using various methods, such as filter methods (e.g., correlation-based), wrapper methods (e.g., recursive feature elimination), or embedded methods (e.g., LASSO regression). Feature selection can help reduce dimensionality by eliminating irrelevant or redundant features, leading to improved model performance and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93bb6b7",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a widely used linear dimensionality reduction technique that transforms the data into a new coordinate system where the axes (principal components) are ordered by the amount of variance they capture from the data. The first principal component captures the most variance, the second captures the second most, and so on.\n",
    "\n",
    "This allows PCA to reduce the dimensionality of the data while retaining as much information as possible. PCA works by computing the covariance matrix of the data, finding its eigenvalues and eigenvectors, and then projecting the data onto the eigenvectors corresponding to the largest eigenvalues.\n",
    "\n",
    "The number of principal components to retain can be chosen based on the cumulative explained variance, which indicates how much of the total variance is captured by the selected components. PCA is particularly effective for linear relationships and can be used for both feature extraction and data visualization.\n",
    "\n",
    "**It is important to note that PCA assumes that the principal components are orthogonal and that the data is centered (mean-centered) before applying PCA.**\n",
    "\n",
    "This means that PCA is sensitive to the scale of the features, and it is often recommended to standardize or normalize the data before applying PCA. PCA can also be used for noise reduction by retaining only the most significant principal components and discarding the ones that capture less variance, which are often associated with noise. This can help improve the performance of machine learning models by reducing overfitting and improving generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17d66d",
   "metadata": {},
   "source": [
    "### Mathematical Formulation\n",
    "\n",
    "Given a dataset $X \\in \\mathbb{R}^{n \\times p}$ with $n$ samples and $p$ features, PCA aims to find a lower-dimensional representation of the data by projecting it onto a new set of axes (principal components). The steps involved in PCA are:\n",
    "\n",
    "1.  **Standardization**: Center the data by subtracting the mean of each feature, $$X' = X - \\bar{X},$$ where $\\bar{X}$ is the mean vector of the features.\n",
    "2.  **Covariance Matrix**: Compute the covariance matrix of the centered data, $$C = \\frac{1}{n-1} X'^T X'.$$\n",
    "3.  **Eigenvalue Decomposition**: Perform eigenvalue decomposition on the covariance matrix to obtain $$C = V \\Lambda V^T,$$ where $V$ is the matrix of eigenvectors and $\\Lambda$ is the diagonal matrix of eigenvalues.\n",
    "4.  **Select Principal Components**: Choose the top $k$ eigenvectors corresponding to the largest eigenvalues to form the projection matrix, $$W_k = [v_1, v_2, \\ldots, v_k],$$ where $v_i$ are the eigenvectors.\n",
    "5.  **Project Data**: Project the original data onto the lower-dimensional space using the projection matrix, $$Z = X' W_k,$$ where $Z \\in \\mathbb{R}^{n \\times k}$ is the reduced representation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e967efb",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages\n",
    "\n",
    "-   **Advantages**:\n",
    "    -   Reduces dimensionality while preserving variance.\n",
    "    -   Helps in noise reduction by discarding less significant components.\n",
    "    -   Can improve the performance of machine learning models by reducing overfitting.\n",
    "-   **Disadvantages**:\n",
    "    -   Assumes linear relationships between features.\n",
    "    -   Sensitive to the scale of features; requires standardization or normalization.\n",
    "    -   May not capture complex, non-linear relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31b15e",
   "metadata": {},
   "source": [
    "### Evaluation of PCA\n",
    "\n",
    "To evaluate the effectiveness of PCA, we can use the explained variance ratio, which indicates how much variance is captured by each principal component. The cumulative explained variance can help determine the number of components to retain for dimensionality reduction. A common approach is to plot the cumulative explained variance against the number of principal components and choose a threshold (e.g., 95% or 99%) to decide how many components to keep. This ensures that we retain enough information while reducing dimensionality.\n",
    "\n",
    "-   **Explained Variance Ratio**: The explained variance ratio for each principal component can be computed as: $$\\text{Explained Variance Ratio}_i = \\frac{\\lambda_i}{\\sum_{j=1}^{p} \\lambda_j},$$ where $\\lambda_i$ is the $i$-th eigenvalue. The ****cumulative explained variance ratio**** can be computed as: $$\\text{Cumulative Explained Variance Ratio}_k = \\sum_{i=1}^{k} \\text{Explained Variance Ratio}_i.$$\n",
    "-   **Scree Plot**: A scree plot can be used to visualize the explained variance for each principal component. It is a line plot where the x-axis represents the principal components and the y-axis represents the explained variance ratio. The \"elbow\" point in the plot can help determine the optimal number of components to retain.\n",
    "-   **Reconstruction Error**: The reconstruction error can be computed as the mean squared error between the original data and the reconstructed data from the reduced representation. This can help assess how well PCA captures the structure of the data. The reconstruction error can be computed as: $$\\text{Reconstruction Error} = \\frac{1}{n} \\|X - Z W_k^T\\|^2,$$ where $Z$ is the reduced representation and $W_k$ is the projection matrix. A lower reconstruction error indicates that PCA has effectively captured the structure of the data.\n",
    "-   **Visualization**: Visualizing the reduced data in 2D or 3D can help assess how well PCA separates different classes or clusters in the data. This can be done using scatter plots or pair plots, where the axes represent the principal components. If the reduced data shows clear separation between classes or clusters, it indicates that PCA has effectively captured the underlying structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42918989",
   "metadata": {},
   "source": [
    "### Example: Synthetic Dataset\n",
    "\n",
    "To illustrate PCA, we can create a synthetic dataset with two features and visualize the results of PCA.\n",
    "\n",
    "-   Generate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ac88eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Create a synthetic dataset with two features\n",
    "np.random.seed(42)\n",
    "X = np.dot(np.random.rand(2, 2), np.random.randn(2, 100)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a176b08a",
   "metadata": {},
   "source": [
    "-   Plot the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7286444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], label='Original Data', alpha=0.75)\n",
    "plt.title('Synthetic Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7750a695",
   "metadata": {},
   "source": [
    "-   Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4dad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b912df",
   "metadata": {},
   "source": [
    "-   Evaluate PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b161eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the principal components\n",
    "print(\"Principal Components:\\n\", pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6586709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print explained variance ratio\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "# Print cumulative explained variance\n",
    "print(\"Cumulative Explained Variance:\", np.cumsum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5df240",
   "metadata": {},
   "source": [
    "-   Visualize the PCA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a87d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], label='PCA Reduced Data', alpha=0.75, color='orange')\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "plt.title('PCA of Synthetic Dataset')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77dfd44",
   "metadata": {},
   "source": [
    "-   Visualize the explained variance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c08cab",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(1, pca.n_components_ + 1),\n",
    "        pca.explained_variance_ratio_,\n",
    "        alpha=0.75, color='blue',\n",
    "        label='Explained Variance Ratio')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance Ratio of PCA')\n",
    "plt.xticks(range(1, pca.n_components_ + 1))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f165b1",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "\n",
    "Autoencoders are a type of neural network used for unsupervised learning, particularly for dimensionality reduction and feature extraction. They consist of two main components: an encoder and a decoder. The encoder compresses the input data into a lower-dimensional representation (latent space), while the decoder reconstructs the original data from this representation.\n",
    "\n",
    "Autoencoders can learn complex, non-linear mappings and are particularly useful for high-dimensional data. They can be trained using backpropagation to minimize the reconstruction error, which is the difference between the original input and the reconstructed output.\n",
    "\n",
    "Autoencoders can be used for various tasks, including:\n",
    "\n",
    "-   Dimensionality reduction: By training the autoencoder to compress the input data into a lower-dimensional representation, we can effectively reduce the dimensionality of the data while retaining important features.\n",
    "-   Feature extraction: The latent space representation learned by the encoder can be used as a new set of features for downstream tasks, such as classification or clustering.\n",
    "-   Denoising: Denoising autoencoders can be trained to reconstruct the original data from noisy inputs, effectively learning to filter out noise and retain important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db1f75",
   "metadata": {},
   "source": [
    "### Mathematical Formulation\n",
    "\n",
    "Autoencoders can be mathematically formulated as follows:\n",
    "\n",
    "1.  **Encoder**: The encoder maps the input data $X \\in \\mathbb{R}^{n \\times p}$ to a lower-dimensional representation $Z \\in \\mathbb{R}^{n \\times k}$ using a function $f$: $$Z = f(X) = \\sigma(W_e X + b_e),$$ where $W_e$ is the weight matrix, $b_e$ is the bias vector, and $\\sigma$ is an activation function (e.g., ReLU, sigmoid).\n",
    "2.  **Decoder**: The decoder reconstructs the original data from the lower-dimensional representation $Z$ using a function $g$: $$\\hat{X} = g(Z) = \\sigma(W_d Z + b_d),$$ where $W_d$ is the weight matrix, $b_d$ is the bias vector, and $\\sigma$ is an activation function.\n",
    "3.  **Loss Function**: The autoencoder is trained to minimize the reconstruction error, which can be defined as the mean squared error (MSE) between the original input $X$ and the reconstructed output $\\hat{X}$: $$\\mathcal{L}(X, \\hat{X}) = \\frac{1}{n} \\sum_{i=1}^{n} \\|X_i - \\hat{X}_i\\|^2,$$ where $X_i$ and $\\hat{X}_i$ are the $i$-th samples of the original input and reconstructed output, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00253ccf",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages\n",
    "\n",
    "-   **Advantages**:\n",
    "    -   Can learn complex, non-linear mappings.\n",
    "    -   Effective for high-dimensional data.\n",
    "    -   Can be used for various tasks, including dimensionality reduction, feature extraction, and denoising.\n",
    "-   **Disadvantages**:\n",
    "    -   Requires a large amount of data for training.\n",
    "    -   Can be computationally expensive, especially for deep autoencoders.\n",
    "    -   May suffer from overfitting if not properly regularized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79bdfa8",
   "metadata": {},
   "source": [
    "### Evaluation of Autoencoders\n",
    "\n",
    "To evaluate the effectiveness of autoencoders, we can use the reconstruction error as a metric. A lower reconstruction error indicates that the autoencoder has effectively learned to compress and reconstruct the input data. Additionally, we can visualize the latent space representation learned by the encoder to assess how well it captures the structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf7d05",
   "metadata": {},
   "source": [
    "## Hands-on Exercises\n",
    "\n",
    "Use a synthetic dataset with 4 highly correlated features and apply PCA to reduce the dimensionality to 2 dimensions. Visualize the results and evaluate the explained variance ratio.\n",
    "\n",
    "-   Generate a synthetic dataset with 4 features\n",
    "-   Visualize the correlations between features\n",
    "-   Apply PCA with 4 components and visualize the results (first 2 PCs)\n",
    "-   Print the principal components and explained variance ratio\n",
    "-   Make a scree plot to visualize the explained variance\n",
    "-   Visualize the correlation between the principal components"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "ML Workshop",
   "language": "python",
   "name": "ml_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
