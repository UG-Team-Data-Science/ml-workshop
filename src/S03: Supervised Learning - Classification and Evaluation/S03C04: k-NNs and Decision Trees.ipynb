{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3753a38",
   "metadata": {},
   "source": [
    "\n",
    "# k-NN & Decision Trees Classification\n",
    "\n",
    "Machine learning classifiers can be broadly categorized into linear and non-linear models. Non-linear classifiers are particularly useful when the relationship between features and target classes is complex or not easily separable by a straight line. Two popular non-linear classifiers are:\n",
    "\n",
    "-   **k-Nearest Neighbors (k-NN)**\n",
    "-   **Decision Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19724184",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors (k-NN)\n",
    "\n",
    "k-Nearest Neighbors is a non-parametric, **instance-based** learning algorithm which does not explicitly learn a model but stores the training instances. For a new observation $\\hat{x}$, the algorithm:\n",
    "\n",
    "1.  Computes the distance between $\\hat{x}$ and every point in the training set (commonly the Euclidean distance, but can also use Manhattan or Minkowski distances).\n",
    "2.  Selects the $k$ closest neighbors.\n",
    "3.  Predicts the class by majority vote among those neighbors.\n",
    "\n",
    "The decision boundary is implicitly defined by the $k$ nearest points; therefore the model can capture highly non-linear class boundaries without explicit training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20299e17",
   "metadata": {},
   "source": [
    "### Mathematical formulation\n",
    "\n",
    "Given a training set $\\{{(x_i, y_i)}\\}_{i = 1}^{n}$ where $y_i \\in \\{1, \\dots,C\\}$, the prediction for a query point $\\hat{x}$ is $$\\hat{y} = \\text{mode}\\left(\\{y_j | x_j \\in N_k(\\hat{x})\\}\\right),$$\n",
    "\n",
    "where $N_k(\\hat{x})$ denotes the set of $k$ training points closest to $\\hat{x}$.\n",
    "\n",
    "-   **Choosing $k$**:\n",
    "    -   Small $k$ can lead to noise sensitivity (overfitting).\n",
    "    -   Large $k$ can smooth out class boundaries (underfitting).\n",
    "    -   Cross-validation is often used to select the optimal $k$.\n",
    "-   **Advantages**:\n",
    "    -   Simple and intuitive.\n",
    "    -   No training phase; all computation happens at prediction time.\n",
    "    -   Naturally handles multi-class classification.\n",
    "-   **Disadvantages**:\n",
    "    -   Computationally expensive at prediction time, especially with large datasets.\n",
    "    -   Sensitive to irrelevant features and feature scaling.\n",
    "    -   Requires a good choice of distance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e14ca",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "A Decision Tree recursively partitions the feature space into axis-aligned regions and assigns a class label to each region. It consists of nodes (features), branches (decisions), and leaves (outcomes). At each node the algorithm chooses the split that maximizes a purity metric such as **information gain** or **Gini impurity**. The tree is built by recursively splitting the dataset into subsets until a stopping criterion is met (e.g., maximum depth, minimum samples per leaf).\n",
    "\n",
    "By hierarchically splitting the space, a tree can approximate complex decision boundaries while remaining interpretable-each internal node corresponds to a human-readable rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924bc683",
   "metadata": {},
   "source": [
    "### Information gain\n",
    "\n",
    "For a node containing a sample set $S$, the entropy is\n",
    "\n",
    "$$H(S)= -\\sum_{c = 1}^{C} p_c \\log_2 p_c,$$\n",
    "\n",
    "with $p_c$ the proportion of class $c$. A split of $S$ into subsets $S_L, S_R$ yields information gain\n",
    "\n",
    "$$IG = H(S) - \\frac{|S_L|}{|S|} H(S_L) - \\frac{|S_R|}{|S|} H(S_R).$$\n",
    "\n",
    "The algorithm selects the split with the highest **IG** (or lowest Gini).\n",
    "\n",
    "-   **Advantages**:\n",
    "    -   Easy to interpret and visualize.\n",
    "    -   Handles both numerical and categorical data.\n",
    "    -   Non-parametric: does not assume any specific distribution of the data.\n",
    "-   **Disadvantages**:\n",
    "    -   Prone to overfitting, especially with deep trees.\n",
    "    -   Sensitive to small changes in the data (can lead to different trees).\n",
    "    -   Can create biased trees if some classes dominate.\n",
    "-   **Pruning**: Techniques like cost-complexity pruning can be applied to reduce overfitting by removing branches that have little importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833fd16e",
   "metadata": {},
   "source": [
    "## Practical Demonstration\n",
    "\n",
    "Below we illustrate both algorithms on the classic **Iris** data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845841c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load data and keep only two features for 2-D visualisation\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, 2:4]\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names[2:4]\n",
    "class_names = iris.target_names\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857bccab",
   "metadata": {},
   "source": [
    "-   Preprocessing the data and training the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d5e3d-ee44-4dd2-a4e6-14df7df29b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline for k-NN containing standardisation and classifier\n",
    "knn_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "\n",
    "# Fit k-NN\n",
    "knn_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate k-NN\n",
    "y_pred_knn = knn_pipeline.predict(X_test)\n",
    "print(\"k-NN accuracy:\", accuracy_score(y_test, y_pred_knn))\n",
    "print(classification_report(y_test, y_pred_knn, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35870096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline for DecisionTree\n",
    "dt_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Non-necessary for DT, but included for consistency\n",
    "    ('dt', DecisionTreeClassifier(max_depth=3, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit Decision Tree\n",
    "dt_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate Decision Tree\n",
    "y_pred_dt = dt_pipeline.predict(X_test)\n",
    "print(\"Decision Tree accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
    "print(classification_report(y_test, y_pred_dt, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff00f238",
   "metadata": {},
   "source": [
    "-   Visualising the decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253f6e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    knn_pipeline, X, response_method='predict',\n",
    "    cmap=plt.cm.coolwarm, alpha=0.85,\n",
    "    xlabel=feature_names[0], ylabel=feature_names[1],\n",
    "    grid_resolution=500)\n",
    "plt.title(\"k-NN Decision Boundary\")\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolor='k', s=20, cmap=plt.cm.coolwarm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d5844",
   "metadata": {},
   "outputs": [],
   "source": [
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    dt_pipeline, X, response_method='predict',\n",
    "    cmap=plt.cm.coolwarm, alpha=0.75,\n",
    "    xlabel=feature_names[0], ylabel=feature_names[1],\n",
    "    grid_resolution=500)\n",
    "plt.title(\"Decision Tree Decision Boundary\")\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolor='k', s=20, cmap=plt.cm.coolwarm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b75b6c",
   "metadata": {},
   "source": [
    "-   Visualising the Decision Tree structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b9ad02",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(dt_pipeline.named_steps['dt'], filled=True, feature_names=feature_names, class_names=class_names)\n",
    "plt.title(\"Decision Tree Structure\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a36d24",
   "metadata": {},
   "source": [
    "## Hands-on Exercises\n",
    "\n",
    "-   **k-NN hyper-parameters**\n",
    "    -   Re-run the demo varying $k$ from 1 to 15.\n",
    "    -   Try Manhattan distance (`metric='manhattan'`) and observe how the boundary changes.\n",
    "-   **Decision Tree pruning**\n",
    "    -   Grow an unpruned tree (`max_depth=None`) and visualise it with `plot_tree`.\n",
    "    -   What, in your opinion, is the disadvantage of an unpruned tree?\n",
    "\n",
    "-   **Multiclass confusion matrix**\n",
    "    -   Implement and plot confusion matrices for both classifiers on the full four-feature Iris data set.\n",
    "        -   Which mis-classifications are most common and why?\n",
    "-   **Real-world data**\n",
    "    -   Load the **Wine** data set (`sklearn.datasets.load_wine`).\n",
    "    -   Explore the data set and visualise the features.\n",
    "    -   Choose two features for training and visualisation (e.g., alcohol and malic acid).\n",
    "    -   Build a pipeline with standardisation + k-NN vs. a depth-limited decision tree for the chosen features.\n",
    "    -   Compare performance using 5-fold cross-validation."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "ML Workshop",
   "language": "python",
   "name": "ml_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
