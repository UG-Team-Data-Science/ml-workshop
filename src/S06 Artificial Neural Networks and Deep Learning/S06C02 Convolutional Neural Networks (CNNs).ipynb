{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931ba52b",
   "metadata": {},
   "source": [
    "\n",
    "# Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a specialized type of neural network primarily used for processing grid-like data, such as images. They are designed to automatically and adaptively learn spatial hierarchies of features from input data, making them particularly effective for tasks like image recognition, object detection, and more.\n",
    "\n",
    "CNNs leverage the spatial structure of images by applying convolutional layers, which use filters to detect patterns and features at various levels of abstraction. This hierarchical feature learning allows CNNs to achieve state-of-the-art performance in many computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c8653",
   "metadata": {},
   "source": [
    "## Key Concepts of CNNs\n",
    "\n",
    "-   **Convolutional Layers**: The core building blocks of CNNs, where filters (also known as kernels) slide over the input data to produce feature maps. Each filter learns to detect specific patterns, such as edges, textures, or shapes.\n",
    "-   **Pooling Layers**: Used to reduce the spatial dimensions of feature maps, retaining the most important information while discarding less significant details. Pooling helps to make the model invariant to small translations and distortions in the input data. Common pooling methods include max pooling and average pooling.\n",
    "-   **Activation Functions**: Non-linear functions applied after each convolutional layer to introduce non-linearity into the model. The Rectified Linear Unit (ReLU) is the most commonly used activation function in CNNs, defined as $ f(x) = \\max(0, x) $. Other activation functions include Sigmoid and Tanh, but ReLU is preferred for its simplicity and effectiveness in deep networks.\n",
    "-   **Fully Connected Layers**: After several convolutional and pooling layers, the high-level reasoning in the neural network is performed by fully connected layers. These layers connect every neuron in one layer to every neuron in the next layer, allowing the model to learn complex relationships between features.\n",
    "-   **Dropout**: A regularization technique used to prevent overfitting by randomly setting a fraction of the input units to zero during training. This forces the network to learn more robust features that are not reliant on any single neuron.\n",
    "-   **Batch Normalization**: A technique to normalize the inputs of each layer, which helps to stabilize the learning process and can lead to faster convergence. It reduces internal covariate shift by normalizing the activations of a layer for each mini-batch.\n",
    "-   **Transfer Learning**: A powerful technique where a pre-trained CNN model (trained on a large dataset like ImageNet) is fine-tuned on a smaller, task-specific dataset. This allows leveraging the learned features from the pre-trained model, significantly reducing training time and improving performance on the new task.\n",
    "-   **Data Augmentation**: A technique to artificially increase the size of the training dataset by applying random transformations (e.g., rotations, translations, flips) to the input images. This helps improve the model's robustness and generalization by exposing it to a wider variety of input variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635b46d",
   "metadata": {},
   "source": [
    "## Example: Building a Simple CNN with Keras\n",
    "\n",
    "To illustrate the concepts of CNNs, we will build a simple convolutional neural network using Keras. We will use the MNIST dataset, a well-known dataset for handwritten digit recognition, to demonstrate how to create, train, and evaluate a CNN model.\n",
    "\n",
    "-   Load and preprocess the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f51f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32') / 255.0\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abad3c3",
   "metadata": {},
   "source": [
    "-   Build the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c450f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(28, 28, 1)),  # Input layer for MNIST images\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.2),  # Regularization to prevent overfitting\n",
    "    Dense(10, activation='softmax')  # Output layer for 10 classes\n",
    "])\n",
    "model.summary()  # Display the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0174f984",
   "metadata": {},
   "source": [
    "-   Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa6486",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c6f46",
   "metadata": {},
   "source": [
    "-   Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16757b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=64,\n",
    "                    validation_split=0.2\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be73ff3b",
   "metadata": {},
   "source": [
    "-   Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3c0981",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4fe899",
   "metadata": {},
   "source": [
    "-   Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f3fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mnist_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa76de",
   "metadata": {},
   "source": [
    "-   Visualize the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20850970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = model.history.history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['loss'], label='Training Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d0d22",
   "metadata": {},
   "source": [
    "-   Use the model on an image and show the image with the predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a74fb-07a2-4bfa-adb2-e405c310922a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Predict the class of the image\n",
    "img_index = 12\n",
    "img = X_test[img_index]\n",
    "label = np.argmax(y_test[img_index])\n",
    "predictions = model.predict(np.expand_dims(img, axis=0))\n",
    "predicted_class = np.argmax(predictions[0])\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"Actual Class: {label}\")\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Predicted Class: {predicted_class}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c788e8c0",
   "metadata": {},
   "source": [
    "## Hands-on Exercises\n",
    "\n",
    "-   Load the CIFAR-10 dataset and preprocess it for training a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ea689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# Load CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "# Convert labels to categorical format\n",
    "y_train, y_test = to_categorical(y_train, num_classes=10), to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a57580d",
   "metadata": {},
   "source": [
    "-   Build a simple CNN model using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c454162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    Input(shape=(32, 32, 3)),\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),  # Regularization to prevent overfitting\n",
    "    Dense(10, activation='softmax')  # Output layer for 10 classes\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2622a",
   "metadata": {},
   "source": [
    "-   Compile the model with an appropriate optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d069cc",
   "metadata": {},
   "source": [
    "-   Train the model on the CIFAR-10 training set and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5229d7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          epochs=16,\n",
    "          batch_size=64,\n",
    "          validation_split=0.2)\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724298ca",
   "metadata": {},
   "source": [
    "-   Save the trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b52bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cifar10_cnn_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0806aba7",
   "metadata": {},
   "source": [
    "-   Visualize the training history (loss and accuracy) using Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066650d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history = model.history.history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['loss'], label='Training Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235b6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement data augmentation techniques to improve model generalization.\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# Create an ImageDataGenerator for data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2\n",
    ")\n",
    "# Fit the generator on the training data\n",
    "datagen.fit(X_train)\n",
    "# Train the model using the augmented data\n",
    "model.fit(datagen.flow(X_train, y_train, batch_size=64),\n",
    "          epochs=10,\n",
    "          validation_split=0.2,\n",
    "          steps_per_epoch=len(X_train) // 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cafd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore transfer learning by using a pre-trained CNN model (e.g., VGG16, ResNet50) and fine-tuning it on the CIFAR-10 dataset.\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "# Load the pre-trained VGG16 model without the top layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "# Add custom layers on top of the base model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')  # Output layer for 10 classes\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model on the CIFAR-10 dataset\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "# Save the fine-tuned model\n",
    "model.save('cifar10_vgg16_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a75110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned filters of the first convolutional layer to understand what features the model is learning.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Get the weights of the first convolutional layer\n",
    "filters, biases = model.layers[0].get_weights()\n",
    "# Normalize the filter values to [0, 1] for visualization\n",
    "filters = (filters - filters.min()) / (filters.max() - filters.min())\n",
    "# Plot the first 16 filters\n",
    "n_filters = 16\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(n_filters):\n",
    "      ax = plt.subplot(4, 4, i + 1)\n",
    "      plt.imshow(filters[:, :, :, i], cmap='viridis')\n",
    "      plt.axis('off')\n",
    "plt.suptitle('Learned Filters of the First Convolutional Layer')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "ML Workshop",
   "language": "python",
   "name": "ml_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
