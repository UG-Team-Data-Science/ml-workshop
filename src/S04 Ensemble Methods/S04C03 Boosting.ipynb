{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdafbbe5",
   "metadata": {},
   "source": [
    "\n",
    "# Boosting\n",
    "\n",
    "Boosting is an ensemble learning technique that combines multiple weak learners to create a strong learner. The main idea is to sequentially train models, where each new model focuses on correcting the errors made by the previous models. This iterative process allows boosting to improve the overall performance of the ensemble.\n",
    "\n",
    "Boosting algorithms typically follow these steps:\n",
    "\n",
    "1.  **Initialize Weights**: Start with equal weights for all training instances.\n",
    "2.  **Train Weak Learner**: Train a weak learner (e.g., a decision stump) on the weighted training data.\n",
    "3.  **Update Weights**: Increase the weights of misclassified instances and decrease the weights of correctly classified instances. This ensures that the next weak learner focuses more on the instances that were previously misclassified.\n",
    "4.  **Combine Models**: Combine the predictions of all weak learners, often using a weighted sum where the weights are determined by the performance of each learner.\n",
    "5.  **Repeat**: Repeat steps 2-4 for a specified number of iterations or until a stopping criterion is met (e.g., no significant improvement in performance).\n",
    "6.  **Final Prediction**: The final prediction is made by combining the predictions of all weak learners, typically using a weighted majority vote for classification or a weighted average for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316be885",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages\n",
    "\n",
    "-   **Advantages**:\n",
    "    -   Can significantly improve the performance of weak learners, often achieving state-of-the-art results.\n",
    "    -   Focuses on difficult instances, allowing the model to learn complex patterns in the data.\n",
    "    -   Can handle a wide range of base learners, including decision trees, linear models, and neural networks.\n",
    "    -   Robust to overfitting, especially with regularization techniques like shrinkage (learning rate) and early stopping.\n",
    "    -   Can be used for both classification and regression tasks.\n",
    "-   **Disadvantages**:\n",
    "    -   Sensitive to noisy data and outliers, as boosting focuses on correcting errors.\n",
    "    -   Requires careful tuning of hyperparameters, such as the number of iterations, learning rate, and base learner complexity.\n",
    "    -   Computationally expensive, especially with large datasets and complex base learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1841b89",
   "metadata": {},
   "source": [
    "## Common Boosting Algorithms\n",
    "\n",
    "-   **AdaBoost (Adaptive Boosting)**: One of the earliest and most popular boosting algorithms. It adjusts the weights of misclassified instances and combines weak learners in a weighted manner.\n",
    "-   **Gradient Boosting**: Builds models sequentially, where each new model is trained to minimize the residual errors of the previous models. It can use various loss functions and is highly flexible.\n",
    "-   **XGBoost (Extreme Gradient Boosting)**: An optimized version of gradient boosting that includes regularization techniques to prevent overfitting and improve performance. It is widely used in machine learning competitions.\n",
    "-   **LightGBM**: A gradient boosting framework that uses a histogram-based approach to speed up training and reduce memory usage. It is particularly effective for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cfd8aa",
   "metadata": {},
   "source": [
    "## AdaBoost Overview\n",
    "\n",
    "AdaBoost, short for Adaptive Boosting, is one of the earliest and most widely used boosting algorithms. It works by combining multiple weak classifiers to create a strong classifier. The key steps in AdaBoost are:\n",
    "\n",
    "1.  **Initialize Weights**: Start with equal weights for all training instances.\n",
    "2.  **Train Weak Learner**: Train a weak learner (e.g., a decision stump) on the weighted training data.\n",
    "3.  **Update Weights**: Increase the weights of misclassified instances and decrease the weights of correctly classified instances. This ensures that the next weak learner focuses more on the instances that were previously misclassified.\n",
    "4.  **Combine Models**: Combine the predictions of all weak learners, where each learner's contribution is weighted by its accuracy.\n",
    "5.  **Repeat**: Repeat steps 2-4 for a specified number of iterations or until a stopping criterion is met (e.g., no significant improvement in performance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0571bb23",
   "metadata": {},
   "source": [
    "### Example of AdaBoost\n",
    "\n",
    "We will demonstrate AdaBoost using the titanic dataset, keeping only the `pclass`, `sex`, and `age` features. We will preprocess the data, train a single Decision Tree, and then use AdaBoost to improve the model's performance.\n",
    "\n",
    "-   Load the titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "titanic = datasets.fetch_openml('titanic', version=1, as_frame=True)\n",
    "df = titanic.frame[['pclass', 'sex', 'age', 'survived']]\n",
    "X = df.drop(columns=['survived'])\n",
    "y = df['survived'].astype('category').cat.codes\n",
    "class_names = ['Not Survived', 'Survived']\n",
    "feature_names = X.columns.tolist()\n",
    "print(f\"Feature names: {feature_names}\")\n",
    "print(f\"Class labels: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783cea64",
   "metadata": {},
   "source": [
    "-   Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d400de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dde241",
   "metadata": {},
   "source": [
    "-   Create a preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9bee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "preprocessing_pipeline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('age', Pipeline(\n",
    "            steps=[\n",
    "                ('imputer', SimpleImputer(strategy='mean')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]\n",
    "        ), ['age']),\n",
    "        ('sex', Pipeline(\n",
    "            steps=[\n",
    "                ('encoder', OneHotEncoder(drop='first', sparse_output=False))\n",
    "            ]\n",
    "        ), ['sex']),\n",
    "        ('pclass', Pipeline(\n",
    "            steps=[\n",
    "                ('enconder', OneHotEncoder(sparse_output=False))\n",
    "            ]\n",
    "        ), ['pclass'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "X_train_transformed = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_test_transformed = preprocessing_pipeline.transform(X_test)\n",
    "\n",
    "feature_names = preprocessing_pipeline.get_feature_names_out()\n",
    "print(f\"Transformed feature names: {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a0ec5",
   "metadata": {},
   "source": [
    "-   Train a single Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c04428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "grid_params = {\n",
    "    'max_depth': [None, 2, 3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(dt_model, grid_params, cv=5, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X_train_transformed, y_train)\n",
    "print(\"Best parameters for Decision Tree:\", grid_search.best_params_)\n",
    "\n",
    "dt_model = grid_search.best_estimator_\n",
    "dt_model.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33343b78",
   "metadata": {},
   "source": [
    "-   Evaluate the single Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c135f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dt = dt_model.predict(X_test_transformed)\n",
    "\n",
    "print(f\"Single Decision Tree Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
    "print(classification_report(y_test, y_pred_dt, target_names=class_names))\n",
    "ConfusionMatrixDisplay.from_estimator(dt_model, X_test_transformed, y_test,\n",
    "                                      display_labels=class_names, cmap='Blues')\n",
    "plt.title('Single Decision Tree Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42a9fc",
   "metadata": {},
   "source": [
    "-   Train an AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c9610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "grid_params_ab = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'estimator__max_depth': [None, 2, 3, 4, 5],\n",
    "    'estimator__min_samples_split': [2, 5, 10],\n",
    "    'estimator__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "grid_search_ab = GridSearchCV(\n",
    "    AdaBoostClassifier(estimator=DecisionTreeClassifier(), random_state=42),\n",
    "    grid_params_ab, cv=5, scoring='accuracy', verbose=1, n_jobs=-1\n",
    ")\n",
    "grid_search_ab.fit(X_train_transformed, y_train)\n",
    "print(f\"Best parameters for AdaBoost: {grid_search_ab.best_params_}\")\n",
    "\n",
    "ab_model = grid_search_ab.best_estimator_\n",
    "ab_model.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b861d9a",
   "metadata": {},
   "source": [
    "-   Evaluate the AdaBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af52704",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ab = ab_model.predict(X_test_transformed)\n",
    "print(f\"AdaBoost Classifier Accuracy: {accuracy_score(y_test, y_pred_ab):.4f}\")\n",
    "print(classification_report(y_test, y_pred_ab, target_names=class_names))\n",
    "ConfusionMatrixDisplay.from_estimator(ab_model, X_test_transformed, y_test,\n",
    "                                      display_labels=class_names, cmap='Blues')\n",
    "plt.title('AdaBoost Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8490ae34",
   "metadata": {},
   "source": [
    "-   Feature importance for AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98360d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "importances = ab_model.feature_importances_\n",
    "indices = importances.argsort()[::-1]\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(\"Feature importances for AdaBoost:\")\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1d28bb",
   "metadata": {},
   "source": [
    "-   Plot feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bfa3dc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X_train_transformed.shape[1]), importances[indices], align='center')\n",
    "plt.xticks(range(X_train_transformed.shape[1]), [feature_names[i] for i in indices],\n",
    "           rotation=45)\n",
    "plt.title('Feature Importances from AdaBoost')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548e9078",
   "metadata": {},
   "source": [
    "## Gradient Boosting Overview\n",
    "\n",
    "Gradient Boosting is a powerful ensemble learning technique that builds models sequentially, where each new model is trained to minimize the residual errors of the previous models. The key steps in Gradient Boosting are:\n",
    "\n",
    "1.  **Initialize Model**: Start with a constant model (e.g., the mean of the target variable for regression).\n",
    "2.  **Compute Residuals**: Calculate the residuals (errors) of the current model on the training data.\n",
    "3.  **Train Weak Learner**: Train a weak learner (e.g., a decision tree) on the residuals, focusing on the instances where the current model performs poorly.\n",
    "4.  **Update Model**: Add the predictions of the weak learner to the current model, scaled by a learning rate (shrinkage).\n",
    "5.  **Repeat**: Repeat steps 2-4 for a specified number of iterations or until a stopping criterion is met (e.g., no significant improvement in performance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcbac8f",
   "metadata": {},
   "source": [
    "### Example of Gradient Boosting\n",
    "\n",
    "-   Train a Gradien Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6449bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "grid_params_gb = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "grid_search_gb = GridSearchCV(gb_model, grid_params_gb, cv=5, n_jobs=-1,\n",
    "                              scoring='accuracy', verbose=1)\n",
    "grid_search_gb.fit(X_train_transformed, y_train)\n",
    "gb_model = grid_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", grid_search_gb.best_params_)\n",
    "gb_model.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f08ee3d",
   "metadata": {},
   "source": [
    "-   Evaluate the Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f4376",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gb = gb_model.predict(X_test_transformed)\n",
    "print(f\"Gradient Boosting Classifier Accuracy: {accuracy_score(y_test, y_pred_gb):.4f}\")\n",
    "print(classification_report(y_test, y_pred_gb, target_names=class_names))\n",
    "ConfusionMatrixDisplay.from_estimator(gb_model, X_test_transformed, y_test,\n",
    "                                      display_labels=class_names, cmap='Blues')\n",
    "plt.title('Gradient Boosting Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229c7869",
   "metadata": {},
   "source": [
    "-   Feature importance for Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032ba3ef",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "importances = gb_model.feature_importances_\n",
    "indices = importances.argsort()[::-1]\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names,\n",
    "                                      'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance',\n",
    "                                                          ascending=False)\n",
    "print(\"Feature importances for Gradient Boosting:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X_train_transformed.shape[1]), importances[indices], align='center')\n",
    "plt.xticks(range(X_train_transformed.shape[1]), [feature_names[i] for i in indices],\n",
    "           rotation=45)\n",
    "plt.title('Feature Importances from Gradient Boosting')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d51af6",
   "metadata": {},
   "source": [
    "## XGBoost Overview\n",
    "\n",
    "XGBoost, short for Extreme Gradient Boosting, is an optimized version of gradient boosting that includes regularization techniques to prevent overfitting and improve performance. It is widely used in machine learning competitions and real-world applications. The key features of XGBoost include:\n",
    "\n",
    "-   **Regularization**: XGBoost includes L1 (Lasso) and L2 (Ridge) regularization to control model complexity and prevent overfitting.\n",
    "-   **Tree Pruning**: XGBoost uses a depth-first approach to grow trees and prunes them based on a minimum loss reduction criterion, which helps in building more efficient models.\n",
    "-   **Parallelization**: XGBoost can parallelize the tree construction process, making it faster than traditional gradient boosting methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7686d58a",
   "metadata": {},
   "source": [
    "### Example of XGBoost\n",
    "\n",
    "-   Train a XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3fff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# XGBoost Classifier\n",
    "grid_params_xgb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_child_weight': [1, 2, 3],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    grid_params_xgb, cv=5, n_jobs=-1, scoring='accuracy', verbose=1\n",
    ")\n",
    "grid_search_xgb.fit(X_train_transformed, y_train)\n",
    "print(\"Best parameters for XGBoost:\", grid_search_xgb.best_params_)\n",
    "xgb_model = grid_search_xgb.best_estimator_\n",
    "xgb_model.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0aba95",
   "metadata": {},
   "source": [
    "-   Evaluate the XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61daade",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "y_pred_xgb = xgb_model.predict(X_test_transformed)\n",
    "print(f\"XGBoost Classifier Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=class_names))\n",
    "ConfusionMatrixDisplay.from_estimator(xgb_model, X_test_transformed, y_test,\n",
    "                                        display_labels=class_names, cmap='Blues')\n",
    "plt.title('XGBoost Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance for XGBoost\n",
    "import pandas as pd\n",
    "importances = xgb_model.feature_importances_\n",
    "indices = importances.argsort()[::-1]\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(\"Feature importances for XGBoost:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X_train_transformed.shape[1]), importances[indices], align='center')\n",
    "plt.xticks(range(X_train_transformed.shape[1]), [feature_names[i] for i in indices],\n",
    "           rotation=45)\n",
    "plt.title('Feature Importances from XGBoost')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a03e45",
   "metadata": {},
   "source": [
    "## Hands-on Exercise\n",
    "\n",
    "-   Implement a boosting algorithm of your choice (AdaBoost, Gradient Boosting, or XGBoost) on the `house_prices` dataset."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "ML Workshop",
   "language": "python",
   "name": "ml_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
