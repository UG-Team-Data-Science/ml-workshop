{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e8bb5b",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction to Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a5787",
   "metadata": {},
   "source": [
    "## What are Ensemble Methods\n",
    "\n",
    "**Ensemble methods** are a powerful class of machine learning techniques that combine multiple models to improve predictive performance. The main idea is to leverage the strengths of different models while mitigating their weaknesses, leading to more robust and accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c2c7b5",
   "metadata": {},
   "source": [
    "## Why Use Ensemble Methods?\n",
    "\n",
    "Ensemble methods are used for several reasons:\n",
    "\n",
    "-   **Improved Accuracy**: They often outperform individual models by reducing variance and bias.\n",
    "-   **Robustness**: Ensembles are less sensitive to noise and outliers, leading to more stable predictions.\n",
    "-   **Reduced Overfitting**: By averaging predictions from multiple models, ensembles can reduce overfitting.\n",
    "-   **Interpretability**: Some ensemble methods, like Random Forests, provide insights into feature importance.\n",
    "-   **Flexibility**: They can be applied to various base models, including decision trees, linear models, and neural networks.\n",
    "-   **Scalability**: Many ensemble methods can be parallelized, making them suitable for large datasets.\n",
    "-   **Combining Different Algorithms**: Ensembles can combine predictions from different types of models, enhancing overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8251800a",
   "metadata": {},
   "source": [
    "## Types of Ensemble Methods\n",
    "\n",
    "Ensemble methods can be broadly categorized into two types:\n",
    "\n",
    "-   **Bagging (Bootstrap Aggregating)**: Combines predictions from multiple models trained on different subsets of the training data. The most common example is Random Forests, which builds multiple decision trees and averages their predictions.\n",
    "-   **Pasting**: Similar to bagging, but uses subsets of the training data without replacement.\n",
    "-   **Boosting**: Sequentially builds models, where each new model focuses on correcting the errors made by the previous models. Examples include AdaBoost, Gradient Boosting, and XGBoost.\n",
    "-   **Stacking**, **Blending**, **Bayesian Model Averaging**, **Voting**, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f6b5c3",
   "metadata": {},
   "source": [
    "## Practical Demonstration\n",
    "\n",
    "To illustrate the concepts of ensemble methods, we will use the Iris dataset and apply both Random Forests and Gradient Boosting classifiers. We will compare their performance using accuracy and confusion matrices.\n",
    "\n",
    "-   Loading the `iris` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e5089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b92e3",
   "metadata": {},
   "source": [
    "-   Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7908436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73739028",
   "metadata": {},
   "source": [
    "-   Train a Random Forest Classifier and evaluate its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c0b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Classifier Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf, target_names=class_names))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(rf_model, X_test, y_test,\n",
    "                                      display_labels=class_names, cmap='Blues')\n",
    "plt.title('Random Forest Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1108bd",
   "metadata": {},
   "source": [
    "-   Train a Gradient Boosting Classifier and evaluate its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5d19ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "print(\"Gradient Boosting Classifier Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
    "print(classification_report(y_test, y_pred_gb, target_names=class_names))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(gb_model, X_test, y_test,\n",
    "                                      display_labels=class_names, cmap='Blues')\n",
    "plt.title('Gradient Boosting Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08ead77",
   "metadata": {},
   "source": [
    "-   Feature importance for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063be6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_model.feature_importances_\n",
    "indices = importances.argsort()[::-1]\n",
    "print(\"Feature importances for Random Forest:\")\n",
    "for i in range(X.shape[1]):\n",
    "    print(f\"{feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X.shape[1]), importances[indices], align='center')\n",
    "plt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=45)\n",
    "plt.title('Feature Importances from Random Forest')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177f61eb",
   "metadata": {},
   "source": [
    "-   Feature importance for Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fda268",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "gb_importances = gb_model.feature_importances_\n",
    "gb_indices = gb_importances.argsort()[::-1]\n",
    "print(\"Feature importances for Gradient Boosting:\")\n",
    "for i in range(X.shape[1]):\n",
    "    print(f\"{feature_names[gb_indices[i]]}: {gb_importances[gb_indices[i]]:.4f}\")\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X.shape[1]), gb_importances[gb_indices], align='center')\n",
    "plt.xticks(range(X.shape[1]), [feature_names[i] for i in gb_indices], rotation=45)\n",
    "plt.title('Feature Importances from Gradient Boosting')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a74f9f",
   "metadata": {},
   "source": [
    "## Hands-on Exercises\n",
    "\n",
    "**Voting Classifier**: Implement a Voting Classifier that combines the predictions of the Random Forest and Gradient Boosting classifiers. Evaluate its performance on the test set.\n",
    "\n",
    "-   Import, instantiate, and train a `VotingClassifier` model from `sklearn.ensemble`.\n",
    "\n",
    "-   Evaluate the performance of the Voting Classifier"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "ML Workshop",
   "language": "python",
   "name": "ml_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
