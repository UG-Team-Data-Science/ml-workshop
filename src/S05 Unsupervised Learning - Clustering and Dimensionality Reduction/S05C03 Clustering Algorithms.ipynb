{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c723c8c",
   "metadata": {},
   "source": [
    "\n",
    "# Clustering Algorithms\n",
    "\n",
    "Unsupervised learning focuses on finding patterns in data without labeled outputs. Clustering is a key unsupervised learning technique that groups similar observations together based on their features. It is exploratory in nature, meaning the results can vary significantly depending on the algorithm, distance metric used, and parameters set. Common clustering algorithms include:\n",
    "\n",
    "-   **k-Means**: Partitions data into k clusters by minimizing the variance within each cluster.\n",
    "-   **Hierarchical Clustering**: Builds a hierarchy of clusters, either agglomerative (bottom-up) or divisive (top-down).\n",
    "-   **DBSCAN**: Density-Based Spatial Clustering of Applications with Noise, which finds clusters of varying shape and density.\n",
    "-   **Gaussian Mixture Models (GMM)**: Assumes data is generated from a mixture of several Gaussian distributions and uses the Expectation-Maximization algorithm to estimate parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11358bb9",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages\n",
    "\n",
    "-   **Advantages**:\n",
    "    -   Can discover hidden patterns and structures in data.\n",
    "    -   Useful for exploratory data analysis and feature engineering.\n",
    "    -   Can handle large datasets without the need for labeled data.\n",
    "-   **Disadvantages**:\n",
    "    -   Results can be difficult to interpret, especially in clustering.\n",
    "    -   No guarantee of finding meaningful patterns; results can be sensitive to the choice of algorithm and parameters.\n",
    "    -   Evaluation of clustering results can be challenging without ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0dd93a",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "k-Means is a popular clustering algorithm that partitions data into k clusters by minimizing the variance within each cluster. It works as follows:\n",
    "\n",
    "-   Initialize $k$ centroids randomly from the data points.\n",
    "-   Assign each data point to the nearest centroid based on a distance metric (commonly Euclidean distance).\n",
    "-   Update the centroids by calculating the mean of all points assigned to each cluster.\n",
    "-   Repeat steps 2 and 3 until convergence (i.e., centroids do not change significantly)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6227d29",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages\n",
    "\n",
    "-   **Advantages**:\n",
    "    -   Simple and efficient for large datasets.\n",
    "    -   Works well when clusters are spherical and of similar size.\n",
    "-   **Disadvantages**:\n",
    "    -   Requires specifying the number of clusters (k) beforehand.\n",
    "    -   Sensitive to outliers and noise.\n",
    "    -   Assumes clusters are spherical and equally sized, which may not hold in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6562fd",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "To illustrate the concepts of unsupervised learning, we will use the Iris dataset and apply PCA for dimensionality reduction and K-Means for clustering.\n",
    "\n",
    "-   Load the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24574b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Iris dataset\n",
    "df = pd.read_csv('../../data/iris.csv', header=0)\n",
    "X = df.drop('species', axis=1)\n",
    "y = df['species'].astype('category').cat.codes\n",
    "\n",
    "sns.pairplot(df, hue='species', palette='viridis', markers=[\"o\", \"s\", \"D\"])\n",
    "plt.suptitle('Iris Dataset Pairplot', y=1.02)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Features')\n",
    "# plt.legend(title='Species', loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc2836",
   "metadata": {},
   "source": [
    "-   Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cfb71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "# Standardize the data\n",
    "scaler = Normalizer()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898447f4",
   "metadata": {},
   "source": [
    "-   Apply PCA for dimensionality reduction to 2 dimensions and visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b86d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "# Apply PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X)\n",
    "# Plot the PCA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.title('PCA of Iris Dataset')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7632f7a",
   "metadata": {},
   "source": [
    "-   Apply K-Means clustering on the PCA-reduced data and visualize the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f2f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X_pca)\n",
    "y_kmeans = kmeans.predict(X_pca)\n",
    "\n",
    "# Plot the K-Means clustering results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_kmeans, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "            s=200, c='red', marker='X', label='Centroids')\n",
    "plt.title('K-Means Clustering on PCA-Reduced Data')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35350457",
   "metadata": {},
   "source": [
    "-   Evaluate the clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a9ebe9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "# Calculate the silhouette score\n",
    "silhouette_avg = silhouette_score(X_pca, y_kmeans)\n",
    "print(f'Silhouette Score: {silhouette_avg:.2f}')\n",
    "\n",
    "# Calculate the adjusted Rand Index (ARI) to compare with true labels\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "ari = adjusted_rand_score(y, y_kmeans)\n",
    "print(f'Adjusted Rand Index: {ari:.2f}')\n",
    "\n",
    "# Calculate the homogeneity score\n",
    "from sklearn.metrics import homogeneity_score\n",
    "homogeneity = homogeneity_score(y, y_kmeans)\n",
    "print(f'Homogeneity Score: {homogeneity:.2f}')\n",
    "\n",
    "# Calculate the completeness score\n",
    "from sklearn.metrics import completeness_score\n",
    "completeness = completeness_score(y, y_kmeans)\n",
    "print(f'Completeness Score: {completeness:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9e6df",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering builds a hierarchy of clusters, either agglomerative (bottom-up) or divisive (top-down). It does not require specifying the number of clusters in advance. The algorithm works as follows:\n",
    "\n",
    "-   Start with each data point as its own cluster.\n",
    "-   Merge the two closest clusters based on a distance metric (e.g., single-linkage, complete-linkage, average-linkage).\n",
    "-   Repeat step 2 until all points are in a single cluster or a stopping criterion is met (e.g., a desired number of clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ec76da",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages\n",
    "\n",
    "-   **Advantages**:\n",
    "    -   No need to predefine the number of clusters.\n",
    "    -   Produces a dendrogram that visualizes the hierarchy of clusters.\n",
    "-   **Disadvantages**:\n",
    "    -   Computationally expensive for large datasets.\n",
    "    -   Sensitive to noise and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fcba6c",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "To illustrate hierarchical clustering, we will use the Iris dataset and visualize the dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f3a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import numpy as np\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "cls = AgglomerativeClustering(linkage='ward', compute_distances=True, n_clusters=3)\n",
    "cls = cls.fit(X_pca)\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "  counts = np.zeros(model.children_.shape[0])\n",
    "  n_samples = len(model.labels_)\n",
    "  for i, merge in enumerate(model.children_):\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "      if child_idx < n_samples:\n",
    "        current_count += 1  # leaf node\n",
    "      else:\n",
    "        current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "  linkage_matrix = \\\n",
    "    np.column_stack([model.children_, model.distances_, counts]).astype(float)\n",
    "\n",
    "  # Plot the corresponding dendrogram\n",
    "  dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "plot_dendrogram(cls, truncate_mode=\"level\", p=3)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Species')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd07f5",
   "metadata": {},
   "source": [
    "-   Evaluate the clustering results using silhouette score and adjusted Rand index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a0e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the silhouette score\n",
    "silhouette_avg = silhouette_score(X_pca, cls.labels_)\n",
    "print(f'Silhouette Score: {silhouette_avg:.2f}')\n",
    "\n",
    "# Calculate the adjusted Rand Index (ARI) to compare with true labels\n",
    "ari = adjusted_rand_score(y, cls.labels_)\n",
    "print(f'Adjusted Rand Index: {ari:.2f}')\n",
    "\n",
    "# Calculate the homogeneity score\n",
    "homogeneity = homogeneity_score(y, cls.labels_)\n",
    "print(f'Homogeneity Score: {homogeneity:.2f}')\n",
    "\n",
    "# Calculate the completeness score\n",
    "completeness = completeness_score(y, cls.labels_)\n",
    "print(f'Completeness Score: {completeness:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715cf171",
   "metadata": {},
   "source": [
    "-   Visualize the clusters formed by hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31af652",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Plot the clusters formed by hierarchical clustering\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cls.labels_, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.title('Hierarchical Clustering on PCA-Reduced Data')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f05a4",
   "metadata": {},
   "source": [
    "## Hands-on Exercises\n",
    "\n",
    "Use K-Means and Hierarchical Clustering on the Breast Cancer Wisconsin dataset\n",
    "\n",
    "-   Load the Breast Cancer Wisconsin dataset\n",
    "-   Apply K-Means clustering on the PCA-reduced data and visualize the clusters\n",
    "-   Evaluate the clustering results using silhouette score and adjusted Rand index\n",
    "-   Visualize the clusters formed by K-Means clustering and compare with true labels\n",
    "-   Apply Hierarchical Clustering on the PCA-reduced data and visualize the clusters\n",
    "-   Evaluate the clustering results using silhouette score and adjusted Rand index\n",
    "-   Compare the clustering results of K-Means and Hierarchical Clustering"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "ML Workshop",
   "language": "python",
   "name": "ml_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
