% Created 2025-06-29 Sun 15:45
% Intended LaTeX compiler: pdflatex
\documentclass[hyperref={pdfpagelabels=false},aspectratio=169]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usetheme{Madrid}
\usecolortheme{beaver}
\useinnertheme{rounded}
\useoutertheme{tree}
\date{\today}
\title{Machine Learning in Python}
\subtitle{Supervised Learning - Regression and Evaluation}
\author[Marocico, Tatar]{Cristian A. Marocico, A. Emin Tatar}
\institute[CIT]{Center for Information Technology\\University of Groningen}
\RequirePackage{pgfcore}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{blkarray}
\usepackage{eurosym}
\usepackage[english]{babel}
\usepackage{xcolor,colortbl}
\usepackage[utf8]{inputenc}
\usepackage[OT1]{fontenc}
\usepackage{multirow}
\usepackage{listings}
\usepackage{fancyvrb}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage[makeroom]{cancel}
\usepackage{bbm}
\newcommand{\infoTitle}[1]{\renewcommand{\givenTitle}{#1}}
\newcommand{\givenTitle}{Info}
\newenvironment{warning}[1][Info]{%
\infoTitle{#1}
\setbeamercolor{block title}{fg=white,bg=red!100!white}%
\setbeamercolor{block body}{bg=red!10!white}
\begin{block}{\givenTitle}}{
\end{block}}
\DeclareMathOperator*{\argmax}{argmax}
\setbeamercovered{transparent}
\usecolortheme{beaver}
\RequirePackage{pgfcore}
\setbeamercovered{transparent=1}
\mode<presentation>{
\usecolortheme{beaver}
\definecolor{rugcolor}{rgb}{0.8,0,0}
\definecolor{darkblue}{rgb}{0.13,0.29,0.53}
\definecolor{darkgreen}{rgb}{0.0,0.43,0.0}
\definecolor{darkyellow}{rgb}{0.0,0.43,0.43}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{Gray}{gray}{0.85}
\beamertemplatenavigationsymbolsempty
\setbeamercolor{item}{fg=rugcolor!80!black}
\setbeamercolor{title}{fg=rugcolor!80!black}
\setbeamercolor{frametitle}{fg=rugcolor!80!black, bg=black!10!white}
% Colors for 'definition' environment
\setbeamercolor*{block title}{fg=white, bg=darkblue}
\setbeamercolor*{block body}{bg=normal text.bg!85!darkblue}
% Color for the 'question' environment
\setbeamercolor*{block title question}{fg=white, bg=darkyellow}
\setbeamercolor*{block body question}{bg=normal text.bg!85!darkyellow}
\setbeamercolor*{palette tertiary}{bg=rugcolor,fg=white}
%HEADER WITH HIGHLIGHTED SECTION NAMES (optional)
\useheadtemplate{%
\vbox{%
%			\vskip1.2pt
%			\pgfuseimage{logo}
%			\vskip1.2pt
\tinycolouredline{rugcolor}{
\color{white}{
% comment the following line if you don't want the section names lines
%					to appear on top
\insertsectionnavigationhorizontal{\paperwidth}{}{\hskip0pt
plus1filll}
%\pgfuseimage{logored}
}
}
%    \tinycolouredline{rugcolor}
{\color{white}{
%% \insertsectionnavigationhorizontal{\paperwidth}{}{
%                \hskip0pt \hfill}
}}
}
}
%FOOTER WITH AUTHOR NAME(S), PAPER TITLE (ABBREVIATED IF SPECIFIED BY \title),
% AND PAGE COUNTER (optional)
%	\usefoottemplate{%
%		\vbox{%
%			\tinycolouredline{rugcolor}{
%				\color{white}{
%					{\insertshortauthor} \hfill \insertshortsubtitle \hfill
%					%\insertdate \hfill%
%					\textsc{\insertframenumber/\inserttotalframenumber}
%         		}
%         	}
%		}
%	}
}
\newtheorem*{props}{Properties}
\newtheorem*{prop}{Property}
\newtheorem*{notation}{Notation}
\newtheorem*{terminology}{Terminology}
\newcolumntype{a}{>{\columncolor{Gray}}c}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\Var}{\mathbb{V}\mathrm{ar}}
\newcommand{\Corr}{\mathbb{C}\mathrm{orr}}
\newcommand{\Cov}{\mathbb{C}\mathrm{ov}}
\newcommand{\Expt}{\mathbb{E}}
\newcommand{\NorDist}{\mathcal{N}}
\newcommand{\ExpDist}{\mathcal{E}\mathrm{xp}}
\newcommand{\GammaDist}{\mathcal{G}\mathrm{amma}}
\newcommand{\BetaDist}{\mathcal{B}\mathrm{eta}}
\newcounter{listCounter}
\newenvironment{question}{%
\setbeamercolor{block title}{bg=orange!70!white,fg=white}
\setbeamercolor{block body}{bg=yellow!10!white}
\begin{block}{Question}
}{%
\end{block}
}
\lstdefinestyle{mystyle}{
language=R,
backgroundcolor=\color{backcolour},
commentstyle=\color{codegreen},
keywordstyle=\color{darkblue}\bfseries,
numberstyle=\tiny\color{codegray},
stringstyle=\color{codepurple},
basicstyle=\ttfamily,
breakatwhitespace=true,
breaklines=true,
%	captionpos=none,
columns=fixed,
keepspaces=true,
numbers=none,
numbersep=5pt,
showspaces=false,
showstringspaces=false,
showtabs=false,
tabsize=2,
basewidth=1.5em,
escapeinside={<@}{@>}
}
\setbeamertemplate{caption}{\raggedright\insertcaption\par}
\setbeamertemplate{blocks}[rounded][shadow=true]
\renewenvironment{definition}[1][Definition]{%
\setbeamercolor{block title}{fg=white,bg=darkblue}
\setbeamercolor{block body}{fg=black,bg=normal text.bg!85!darkblue}
\begin{block}{#1\hfill \footnotesize{Definition}}
\vspace*{-5pt}
}{%
\end{block}
}
\renewenvironment{example}[1][Example]{%
% Color for the 'example' environment
\setbeamercolor*{block title}{fg=white, bg=darkgreen}
\setbeamercolor*{block body}{bg=normal text.bg!85!darkgreen}
\begin{block}{#1\hfill \footnotesize{Example}}
\vspace*{-5pt}
}{%
\end{block}
}
\renewenvironment{theorem}[1][Theorem]{%
\setbeamercolor*{block title}{fg=white, bg=darkyellow}
\setbeamercolor*{block body}{bg=normal text.bg!85!darkyellow}
\begin{block}{#1\hfill \footnotesize{Theorem}}
\vspace*{-5pt}
}{%
\end{block}
}
\date[Jul 2\textsuperscript{nd} 2025]{Wednesday, July 2\textsuperscript{nd} 2025}
\usepackage{mathtools}
\newcommand{\intsum}{\mathop{\mathrlap{\raisebox{0.1ex}{\hspace{0.2em}$\textstyle\sum$}}\int}\limits}
\setbeamercovered{transparent=0}
\usepackage[timeinterval=60]{tdclock}
\hypersetup{
 pdfauthor={},
 pdftitle={Machine Learning in Python},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 30.1 (Org mode 9.7.11)}, 
 pdflang={English}}
\begin{document}

\maketitle
\begin{frame}{Outline}
\setcounter{tocdepth}{1}
\tableofcontents
\end{frame}

\section{Introduction to Regression}
\label{sec:orge6975e9}
\begin{frame}[label={sec:orgdb36f81}]{Introduction to Regression}
\begin{definition}[Regression]\label{sec:org32f65a0}
\pause
\alert{Regression} is a statistical method used to model the relationship between a dependent variable and one or more independent variables.
\end{definition}
\end{frame}
\section{Simple Linear Regression}
\label{sec:org53eaa09}
\begin{frame}[label={sec:orge480cd1}]{Simple Linear Regression}
\begin{definition}[Simple Linear Regression]\label{sec:org0edc089}
\pause
\alert{Simple Linear Regression} is a method to model the relationship between two variables by fitting a linear equation to observed data.
\pause
Mathematically:
$$y = \beta_0 + \beta_1 x + \epsilon$$
where:
\begin{itemize}
\item \(y\) is the dependent variable (response).
\item \(x\) is the independent variable (predictor).
\item \(\beta_0\) is the y-intercept (constant term).
\item \(\beta_1\) is the slope of the line (coefficient).
\item \(\epsilon\) is the error term (residuals).
\end{itemize}
\end{definition}
\end{frame}
\begin{frame}[label={sec:orgd722b82}]{Simple Linear Regression}
A Simple Linear Regression Machine Learning model will learn the coefficients \(\beta_0\) and \(\beta_1\) from the training data to minimize the difference between the predicted values and the actual values.
\end{frame}
\begin{frame}[label={sec:org52b7732}]{Assumptions of Simple Linear Regression}
\begin{itemize}[<+->]
\item Linearity: The relationship between the independent and dependent variable is linear.
\item Independence: Observations are independent of each other.
\item Homoscedasticity: Constant variance of the error terms.
\item Normality: The residuals (errors) of the model are normally distributed.
\end{itemize}
\end{frame}
\section{Evaluation Metrics for Regression}
\label{sec:orga9686d0}
\begin{frame}[label={sec:org6b784fb}]{Evaluation Metrics for Regression}
\begin{definition}[Common Metrics for Regression]\label{sec:orged54404}
\pause
Common metrics to evaluate regression models include:
\begin{itemize}
\item \alert{Mean Absolute Error (MAE)}
\item \alert{Mean Squared Error (MSE)}
\item \alert{Root Mean Squared Error (RMSE)}
\item \alert{R-squared (\(R^2\))}
\item \alert{Adjusted R-squared}
\end{itemize}
\end{definition}
\end{frame}
\begin{frame}[label={sec:org78c2d67}]{Mean Absolute Error (MAE)}
\begin{definition}[Mean Absolute Error (MAE)]\label{sec:org7671364}
\pause
\alert{Mean Absolute Error (MAE)} is the average of the absolute differences between the predicted and the actual values:
$$\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$$
\pause
where:
\begin{itemize}
\item \(n\) is the number of observations.
\item \(y_i\) is the actual value.
\item \(\hat{y}_i\) is the predicted value.
\end{itemize}
\pause
MAE is a linear score, which can be used when all errors are equally important; it is also less sensitive to outliers compared to MSE.
\end{definition}
\end{frame}
\begin{frame}[label={sec:org70087e1}]{Mean Squared Error (MSE)}
\begin{definition}[Mean Squared Error (MSE)]\label{sec:org1770aa0}
\pause
\alert{Mean Squared Error (MSE)} is the average of the squared differences between the predicted and the actual values:
$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$
\pause
where:
\begin{itemize}
\item \(n\) is the number of observations.
\item \(y_i\) is the actual value.
\item \(\hat{y}_i\) is the predicted value.
\end{itemize}
\pause
MSE is more sensitive to outliers than MAE because it squares the errors, which can disproportionately affect the metric if there are large errors; however, it is useful when larger errors are more significant.
\end{definition}
\end{frame}
\begin{frame}[label={sec:org2e135b2}]{Root Mean Squared Error (RMSE)}
\begin{definition}[Root Mean Squared Error (RMSE)]\label{sec:org8727089}
\pause
\alert{Root Mean Squared Error (RMSE)} is the square root of the average of the squared differences between the predicted and the actual values:
$$\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$
\pause
where:
\begin{itemize}
\item \(n\) is the number of observations.
\item \(y_i\) is the actual value.
\item \(\hat{y}_i\) is the predicted value.
\end{itemize}
\pause
RMSE is in the same units as the dependent variable, making it interpretable; it is also sensitive to outliers, similar to MSE.
\end{definition}
\end{frame}
\begin{frame}[label={sec:orgefb7399}]{R-squared (\(R^2\))}
\begin{definition}[R-squared (\(R^2\))]\label{sec:org26e00db}
\pause
\alert{R-squared (\(R^2\))} is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model:
$$R^2 = 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}}$$
where:
\begin{itemize}
\item \(\text{SS}_{\text{res}}\) is the sum of squares of residuals (errors).
\item \(\text{SS}_{\text{tot}}\) is the total sum of squares (variance of the dependent variable).
\end{itemize}
\pause
\(R^2\) values range from 0 to 1, where:
\begin{itemize}
\item 0 indicates that the model does not explain any of the variability of the response data around its mean.
\item 1 indicates that the model explains all the variability of the response data around its mean.
\end{itemize}
\end{definition}
\end{frame}
\begin{frame}[label={sec:org05e1c60}]{Adjusted R-squared}
\begin{definition}[Adjusted R-squared]\label{sec:orga0e2fe6}
\pause
\alert{Adjusted R-squared} adjusts the \(R^2\) value for the number of predictors in the model, providing a more accurate measure when comparing models with different numbers of predictors:
$$\text{Adjusted } R^2 = 1 - \left(1 - R^2\right) \frac{n - 1}{n - p - 1}$$
where:
\begin{itemize}
\item \(n\) is the number of observations.
\item \(p\) is the number of predictors in the model.
\end{itemize}
\pause
Adjusted \(R^2\) can be negative, which indicates that the model is worse than a horizontal line (mean of the dependent variable); it is useful for comparing models with different numbers of predictors.
\end{definition}
\end{frame}
\section{Robust Regression}
\label{sec:orgd2eac26}
\begin{frame}[label={sec:orgfe1a973}]{Robust Regression}
\begin{definition}[Robust Regression]\label{sec:org70160ef}
\pause
\alert{Robust Regression} is a type of regression analysis designed to be less sensitive to outliers in the data. It provides a more reliable estimate of the relationship between variables when the data contains outliers or violations of assumptions.
\end{definition}
\end{frame}
\begin{frame}[label={sec:orgf899615}]{Types of Robust Regression}
\begin{definition}[Types of Robust Regression]\label{sec:org8f1b998}
There are several types of robust regression techniques, including:
\pause
\begin{itemize}[<+->]
\item \alert{Huber Regression}
\item \alert{RANSAC (RANdom SAmple Consensus)}
\item Least Trimmed Squares (LTS)
\item Theil-Sen Estimator
\item Quantile Regression
\end{itemize}
\end{definition}
\end{frame}
\begin{frame}[label={sec:org472a544}]{Huber Regression}
\begin{definition}[Huber Regression]\label{sec:org00a5f8b}
\pause
\alert{Huber Regression} is a robust regression technique that uses a loss function that is quadratic for small residuals and linear for large residuals. This makes it less sensitive to outliers compared to traditional least squares regression.
\pause
Mathematically, the Huber loss function is defined as:
$$L(y, \hat{y}) = \begin{cases}
\frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \leq \delta \\
\delta |y - \hat{y}| - \frac{1}{2}\delta^2 & \text{if } |y - \hat{y}| > \delta
\end{cases}$$
where \(\delta\) is a threshold that determines the point at which the loss function transitions from quadratic to linear.
\end{definition}
\end{frame}
\begin{frame}[label={sec:org3e4e6ed}]{RANSAC (RANdom SAmple Consensus)}
\begin{definition}[RANSAC (RANdom SAmple Consensus)]\label{sec:orgec26709}
\pause
\alert{RANSAC (RANdom SAmple Consensus)} is an iterative method used to estimate parameters of a mathematical model from a dataset that contains outliers. It works by randomly selecting a subset of the data, fitting a model to this subset, and then determining how many points from the entire dataset fit this model well.
\pause
The RANSAC algorithm consists of the following steps:
\begin{enumerate}
\item Randomly select a subset of the data points.
\item Fit a model to this subset.
\item Determine the inliers (points that fit the model well) and outliers (points that do not fit the model).
\item Repeat steps 1-3 for a specified number of iterations or until a satisfactory model is found.
\item Select the model with the highest number of inliers as the final model.
\end{enumerate}
\end{definition}
\end{frame}
\section{Multiple Linear Regression}
\label{sec:org6ec06e0}
\begin{frame}[label={sec:org124619d}]{Multiple Linear Regression}
\begin{definition}[Multiple Linear Regression]\label{sec:org941af03}
\pause
\alert{Multiple Linear Regression} is an extension of simple linear regression that models the relationship between a dependent variable and multiple independent variables. It is used when there are two or more predictors.
\pause
Mathematically, it is represented as:
$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p + \epsilon$$
where:
\begin{itemize}
\item \(y\) is the dependent variable.
\item \(x_1, x_2, \ldots, x_p\) are the independent variables (predictors).
\item \(\beta_0\) is the y-intercept (constant term).
\item \(\beta_1, \beta_2, \ldots, \beta_p\) are the coefficients (slopes) for each independent variable.
\item \(\epsilon\) is the error term (residuals).
\end{itemize}
\end{definition}
\end{frame}
\begin{frame}[label={sec:org7db860e}]{Assumptions of Multiple Linear Regression}
\begin{definition}[Assumptions of Multiple Linear Regression]\label{sec:org587a982}
\pause
The assumptions of multiple linear regression are similar to those of simple linear regression:
\begin{itemize}
\item Linearity: The relationship between the independent variables and the dependent variable is linear.
\item Independence: Observations are independent of each other.
\item Homoscedasticity: Constant variance of the error terms.
\item Normality: The residuals (errors) of the model are normally distributed.
\item No multicollinearity: The independent variables are not highly correlated with each other.
\end{itemize}
\end{definition}
\end{frame}
\section{Regularized Regression}
\label{sec:orgf30ff21}
\begin{frame}[label={sec:org9d847cd}]{How do Machine Learning Models Learn?}
\begin{definition}[How do Machine Learning Models Learn?]\label{sec:org153c82f}
\pause
Machine Learning models learn by adjusting their parameters (coefficients) to minimize the difference between the predicted values and the actual values in the training data, also known as the \alert{cost function} or \alert{loss function}.
\pause
This process is often done using optimization algorithms that iteratively update the parameters based on the loss function, which measures the error of the predictions.
\end{definition}
\end{frame}
\begin{frame}[label={sec:orga1862bb}]{Cost Function}
\begin{definition}[Cost Function]\label{sec:org1660851}
\pause
The \alert{cost function} (or \alert{loss function}) is a mathematical function that quantifies the difference between the predicted values and the actual values in the training data. It is used to guide the learning process of the model.
\pause
Common cost functions for regression include:
\begin{itemize}
\item \alert{Mean Squared Error (MSE)}
\item \alert{Mean Absolute Error (MAE)}
\end{itemize}
\end{definition}
\end{frame}
\begin{frame}[label={sec:org094e78e}]{Cost Function: Mean Squared Error (MSE)}
\begin{definition}[Cost Function: Mean Squared Error (MSE)]\label{sec:orgaf4ea6f}
\pause
\alert{Mean Squared Error (MSE)} is a common cost function used in regression tasks. It measures the average of the squares of the errors, which are the differences between the predicted values and the actual values:
$$J_\text{MSE}(w) = \frac{1}{2m} \sum_{i=1}^{m} (f(x_i, w) - y_i)^2$$
where:
\begin{itemize}
\item \(m\) is the number of observations.
\item \(f(x_i, w)\) is the predicted value for the \(i\)-th observation given the input features \(x_i\) and model parameters \(w\).
\item \(y_i\) is the actual value for the \(i\)-th observation.
\end{itemize}
\pause
MSE is sensitive to outliers, and is, therefore, useful when larger errors are more significant.
\end{definition}
\end{frame}
\begin{frame}[label={sec:orgb5d058e}]{Cost Function: Mean Absolute Error (MAE)}
\begin{definition}[Cost Function: Mean Absolute Error (MAE)]\label{sec:org4547a93}
\pause
\alert{Mean Absolute Error (MAE)} is another common cost function used in regression tasks. It measures the average of the absolute differences between the predicted values and the actual values:
$$J_\text{MAE}(w) = \frac{1}{m} \sum_{i=1}^{m} |f(x_i, w) - y_i|$$
where:
\begin{itemize}
\item \(m\) is the number of observations.
\item \(f(x_i, w)\) is the predicted value for the \(i\)-th observation given the input features \(x_i\) and model parameters \(w\).
\item \(y_i\) is the actual value for the \(i\)-th observation.
\end{itemize}
\pause
MAE is less sensitive to outliers compared to MSE; useful when all errors are equally important.
\end{definition}
\end{frame}
\begin{frame}[label={sec:org8c9cd0c}]{Gradient Descent}
\begin{definition}[Gradient Descent]\label{sec:org96a7cec}
\pause
\alert{Gradient Descent} is an optimization algorithm used to minimize the cost function by iteratively updating the model parameters in the direction of the steepest descent of the cost function.
\pause
The update rule for gradient descent is:
$$w := w - \alpha \nabla J(w)$$
where:
\begin{itemize}
\item \(w\) is the vector of model parameters (coefficients).
\item \(\alpha\) is the learning rate, which controls the step size of each update.
\item \(\nabla J(w)\) is the gradient of the cost function with respect to the parameters \(w\).
\end{itemize}
\pause
Gradient descent continues until convergence, which is when the change in the cost function is below a certain threshold or a maximum number of iterations is reached.
\end{definition}
\end{frame}
\begin{frame}[label={sec:orged6107d}]{Gradient Descent Algorithm}
\begin{definition}[Gradient Descent Algorithm]\label{sec:org2d5b8e8}
\pause
The \alert{Gradient Descent Algorithm} consists of the following steps:
\begin{enumerate}[<+->]
\item Initialize the model parameters (coefficients) randomly or to zero.
\item Compute the predicted values using the current parameters.
\item Calculate the cost function (loss) using the predicted values and actual values.
\item Compute the gradient of the cost function with respect to the parameters.
\item Update the parameters using the gradient descent update rule.
\item Repeat steps 2-5 until convergence (i.e., the change in the cost function is below a threshold or a maximum number of iterations is reached).
\end{enumerate}
\end{definition}
\end{frame}
\begin{frame}[label={sec:orgdde292a}]{Overfitting and Underfitting}
\begin{definition}[Overfitting and Underfitting]\label{sec:orgf6b88d3}
\pause
\alert{Overfitting} occurs when a model learns the noise in the training data instead of the underlying pattern, resulting in poor generalization to unseen data. It typically happens when the model is too complex relative to the amount of training data.

\pause
\alert{Underfitting} occurs when a model is too simple to capture the underlying pattern in the data, resulting in poor performance on both training and unseen data. It typically happens when the model is not complex enough relative to the data.

\pause
To mitigate overfitting, techniques such as \alert{regularization}, \alert{cross-validation}, and \alert{pruning} can be used. To address underfitting, more complex models or additional features can be considered.
\end{definition}
\end{frame}
\begin{frame}[label={sec:org56db193}]{Regularization}
\begin{definition}[Regularization]\label{sec:org6c5c062}
\pause
\alert{Regularization} is a technique used in machine learning to prevent overfitting by adding a penalty term to the cost function. This penalty discourages overly complex models by constraining the size of the model parameters (coefficients).
\pause
\end{definition}
\end{frame}
\begin{frame}[label={sec:org06071c0}]{Types of Regularized Regression}
\begin{definition}[Types of Regularized Regression]\label{sec:org4279689}
There are two common types of regularized regression:
\begin{itemize}
\item \alert{Ridge Regression (L2 Regularization)}
\item \alert{Lasso Regression (L1 Regularization)}
\end{itemize}
\end{definition}
\end{frame}
\begin{frame}[label={sec:org2194bb2}]{Lasso Regression (L1 Regularization)}
\begin{definition}[Lasso Regression (L1 Regularization)]\label{sec:org0dc8375}
\pause
\alert{Lasso Regression} adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function. The objective function for lasso regression is:
$$J_\text{Lasso}(w) = \frac{1}{2m} \sum_{i=1}^{m} (f(x_i, w) - y_i)^2 + \lambda \sum_{j=1}^{p} |w_j|$$
where:
\begin{itemize}
\item \(\lambda\) is the regularization parameter that controls the strength of the penalty.
\item \(w_j\) are the coefficients of the independent variables.
\end{itemize}
\pause
Lasso regression can shrink some coefficients to exactly zero, effectively performing variable selection and resulting in a simpler model.
\end{definition}
\end{frame}
\begin{frame}[label={sec:org314038a}]{Ridge Regression (L2 Regularization)}
\begin{definition}[Ridge Regression (L2 Regularization)]\label{sec:org65afd49}
\pause
\alert{Ridge Regression} adds a penalty equal to the square of the magnitude of coefficients to the loss function. The objective function for ridge regression is:
$$J_\text{Ridge}(w) = \frac{1}{2m} \sum_{i=1}^{m} (f(x_i, w) - y_i)^2 + \frac{\lambda}{2m} \sum_{j=1}^{p} w_j^2$$
where:
\begin{itemize}
\item \(\lambda\) is the regularization parameter that controls the strength of the penalty.
\item \(w_j\) are the coefficients of the independent variables.
\end{itemize}
\pause
Ridge regression helps to reduce model complexity and multicollinearity by shrinking the coefficients towards zero, but it does not set any coefficients exactly to zero.
\end{definition}
\end{frame}
\section{Nonlinearities in Regression}
\label{sec:org2785429}
\begin{frame}[label={sec:org053a7de}]{Nonlinearities in Regression}
\begin{definition}[Nonlinearities in Regression]\label{sec:org9e0751b}
\pause
\alert{Nonlinearities in Regression} refers to the presence of nonlinear relationships between the independent and dependent variables in a regression model. In such cases, a simple linear regression model may not adequately capture the underlying relationship, leading to poor predictions and insights.
\pause
Nonlinear regression models can help address this limitation by allowing for more flexible relationships between the independent and dependent variables.
\end{definition}
\end{frame}
\begin{frame}[label={sec:org1337e67}]{Types of Nonlinear Regression Models}
\begin{definition}[Types of Nonlinear Regression Models]\label{sec:org5b155c2}
There are several types of nonlinear regression models, including:
\begin{itemize}[<+->]
\item \alert{Polynomial Regression}: Fits a polynomial equation to the data, allowing for curves and bends in the relationship between the independent and dependent variables. The degree of the polynomial can be adjusted to capture more complex relationships.
\item \alert{Exponential Regression}: Fits an exponential function to the data, which is useful for modeling relationships where the rate of change increases or decreases exponentially.
\item \alert{Logarithmic Regression}: Fits a logarithmic function to the data, which is useful for modeling relationships where the rate of change decreases as the independent variable increases.
\end{itemize}
\end{definition}
\end{frame}
\end{document}
