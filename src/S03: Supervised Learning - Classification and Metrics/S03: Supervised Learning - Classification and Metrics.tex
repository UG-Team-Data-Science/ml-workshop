% Created 2025-07-01 Tue 13:44
% Intended LaTeX compiler: pdflatex
\documentclass[hyperref={pdfpagelabels=false},aspectratio=169]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usetheme{Madrid}
\usecolortheme{beaver}
\useinnertheme{rounded}
\useoutertheme{tree}
\date{\today}
\title{Machine Learning in Python}
\subtitle{Supervised Learning - Classification and Metrics}
\author[Marocico, Tatar]{Cristian A. Marocico, A. Emin Tatar}
\institute[CIT]{Center for Information Technology\\University of Groningen}
\RequirePackage{pgfcore}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{blkarray}
\usepackage{eurosym}
\usepackage[english]{babel}
\usepackage{xcolor,colortbl}
\usepackage[utf8]{inputenc}
\usepackage[OT1]{fontenc}
\usepackage{multirow}
\usepackage{listings}
\usepackage{fancyvrb}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage[makeroom]{cancel}
\usepackage{bbm}
\newcommand{\infoTitle}[1]{\renewcommand{\givenTitle}{#1}}
\newcommand{\givenTitle}{Info}
\newenvironment{warning}[1][Info]{%
\infoTitle{#1}
\setbeamercolor{block title}{fg=white,bg=red!100!white}%
\setbeamercolor{block body}{bg=red!10!white}
\begin{block}{\givenTitle}}{
\end{block}}
\DeclareMathOperator*{\argmax}{argmax}
\setbeamercovered{transparent}
\usecolortheme{beaver}
\RequirePackage{pgfcore}
\setbeamercovered{transparent=1}
\mode<presentation>{
\usecolortheme{beaver}
\definecolor{rugcolor}{rgb}{0.8,0,0}
\definecolor{darkblue}{rgb}{0.13,0.29,0.53}
\definecolor{darkgreen}{rgb}{0.0,0.43,0.0}
\definecolor{darkyellow}{rgb}{0.0,0.43,0.43}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{Gray}{gray}{0.85}
\beamertemplatenavigationsymbolsempty
\setbeamercolor{item}{fg=rugcolor!80!black}
\setbeamercolor{title}{fg=rugcolor!80!black}
\setbeamercolor{frametitle}{fg=rugcolor!80!black, bg=black!10!white}
% Colors for 'definition' environment
\setbeamercolor*{block title}{fg=white, bg=darkblue}
\setbeamercolor*{block body}{bg=normal text.bg!85!darkblue}
% Color for the 'question' environment
\setbeamercolor*{block title question}{fg=white, bg=darkyellow}
\setbeamercolor*{block body question}{bg=normal text.bg!85!darkyellow}
\setbeamercolor*{palette tertiary}{bg=rugcolor,fg=white}
%HEADER WITH HIGHLIGHTED SECTION NAMES (optional)
\useheadtemplate{%
\vbox{%
%			\vskip1.2pt
%			\pgfuseimage{logo}
%			\vskip1.2pt
\tinycolouredline{rugcolor}{
\color{white}{
% comment the following line if you don't want the section names lines
%					to appear on top
\insertsectionnavigationhorizontal{\paperwidth}{}{\hskip0pt
plus1filll}
%\pgfuseimage{logored}
}
}
%    \tinycolouredline{rugcolor}
{\color{white}{
%% \insertsectionnavigationhorizontal{\paperwidth}{}{
%                \hskip0pt \hfill}
}}
}
}
%FOOTER WITH AUTHOR NAME(S), PAPER TITLE (ABBREVIATED IF SPECIFIED BY \title),
% AND PAGE COUNTER (optional)
%	\usefoottemplate{%
%		\vbox{%
%			\tinycolouredline{rugcolor}{
%				\color{white}{
%					{\insertshortauthor} \hfill \insertshortsubtitle \hfill
%					%\insertdate \hfill%
%					\textsc{\insertframenumber/\inserttotalframenumber}
%         		}
%         	}
%		}
%	}
}
\newtheorem*{props}{Properties}
\newtheorem*{prop}{Property}
\newtheorem*{notation}{Notation}
\newtheorem*{terminology}{Terminology}
\newcolumntype{a}{>{\columncolor{Gray}}c}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\Var}{\mathbb{V}\mathrm{ar}}
\newcommand{\Corr}{\mathbb{C}\mathrm{orr}}
\newcommand{\Cov}{\mathbb{C}\mathrm{ov}}
\newcommand{\Expt}{\mathbb{E}}
\newcommand{\NorDist}{\mathcal{N}}
\newcommand{\ExpDist}{\mathcal{E}\mathrm{xp}}
\newcommand{\GammaDist}{\mathcal{G}\mathrm{amma}}
\newcommand{\BetaDist}{\mathcal{B}\mathrm{eta}}
\newcounter{listCounter}
\newenvironment{question}{%
\setbeamercolor{block title}{bg=orange!70!white,fg=white}
\setbeamercolor{block body}{bg=yellow!10!white}
\begin{block}{Question}
}{%
\end{block}
}
\lstdefinestyle{mystyle}{
language=R,
backgroundcolor=\color{backcolour},
commentstyle=\color{codegreen},
keywordstyle=\color{darkblue}\bfseries,
numberstyle=\tiny\color{codegray},
stringstyle=\color{codepurple},
basicstyle=\ttfamily,
breakatwhitespace=true,
breaklines=true,
%	captionpos=none,
columns=fixed,
keepspaces=true,
numbers=none,
numbersep=5pt,
showspaces=false,
showstringspaces=false,
showtabs=false,
tabsize=2,
basewidth=1.5em,
escapeinside={<@}{@>}
}
\setbeamertemplate{caption}{\raggedright\insertcaption\par}
\setbeamertemplate{blocks}[rounded][shadow=true]
\renewenvironment{definition}[1][Definition]{%
\setbeamercolor{block title}{fg=white,bg=darkblue}
\setbeamercolor{block body}{fg=black,bg=normal text.bg!85!darkblue}
\begin{block}{#1\hfill \footnotesize{Definition}}
\vspace*{-5pt}
}{%
\end{block}
}
\renewenvironment{example}[1][Example]{%
% Color for the 'example' environment
\setbeamercolor*{block title}{fg=white, bg=darkgreen}
\setbeamercolor*{block body}{bg=normal text.bg!85!darkgreen}
\begin{block}{#1\hfill \footnotesize{Example}}
\vspace*{-5pt}
}{%
\end{block}
}
\renewenvironment{theorem}[1][Theorem]{%
\setbeamercolor*{block title}{fg=white, bg=darkyellow}
\setbeamercolor*{block body}{bg=normal text.bg!85!darkyellow}
\begin{block}{#1\hfill \footnotesize{Theorem}}
\vspace*{-5pt}
}{%
\end{block}
}
\date[Jul 4\textsuperscript{th} 2025]{Friday, July 4\textsuperscript{th} 2025}
\usepackage{mathtools}
\newcommand{\intsum}{\mathop{\mathrlap{\raisebox{0.1ex}{\hspace{0.2em}$\textstyle\sum$}}\int}\limits}
\setbeamercovered{transparent=0}
\usepackage[timeinterval=60]{tdclock}
\hypersetup{
 pdfauthor={},
 pdftitle={Machine Learning in Python},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 30.1 (Org mode 9.7.11)}, 
 pdflang={English}}
\begin{document}

\maketitle
\begin{frame}{Outline}
\setcounter{tocdepth}{1}
\tableofcontents
\end{frame}

\section{Introduction to Classification}
\label{sec:org4ab6482}
\begin{frame}[label={sec:orgb0a1296}]{Introduction to Classification}
\begin{definition}[Classification]\label{sec:orge1a3c8b}
\pause
\alert{Classification} is a type of supervised learning where the model learns from labeled data to predict the class of new observations based on past data.
\pause

\alert{Classification vs. Regression} is a key distinction in supervised learning:
\begin{itemize}[<+->]
\item In \alert{classification}, the target variable is categorical (e.g., "spam" or "not spam").
\item In \alert{regression}, the target variable is continuous (e.g., predicting a price).
\end{itemize}
\end{definition}
\end{frame}
\begin{frame}[label={sec:orgda398e2}]{Classification Types}
\begin{definition}[Classification Types]\label{sec:org540d339}
\pause
\alert{Classification} can be broadly divided into two types:
\begin{itemize}[<+->]
\item \alert{Binary Classification}: The target variable has two classes (e.g., "yes" or "no", "spam" or "not spam"). Numerically, this can always be represented as 0 and 1.
\item \alert{Multiclass Classification}: The target variable has more than two classes (e.g., "cat", "dog", "weasel"). In this case, the model predicts one of several possible categories.
\end{itemize}
\end{definition}
\end{frame}
\begin{frame}[label={sec:org07952b3}]{Classification Algorithms}
\begin{definition}[Classification Algorithms]\label{sec:org57f7698}
\pause
\alert{Classification algorithms} are designed to learn from labeled data and make predictions about the class of new, unseen data. Some common algorithms include:
\begin{itemize}[<+->]
\item \alert{Logistic Regression}: Despite its name, it is used for binary classification. It models the probability that a given input belongs to a particular class.
\item \alert{k-Nearest Neighbors (k-NN)}: A non-parametric method that classifies a data point based on the classes of its nearest neighbors in the feature space.
\item \alert{Decision Trees}: A tree-like model that splits the data into subsets based on feature values, leading to a decision about the class label.
\item \alert{Support Vector Machines (SVM)}: A method that finds the hyperplane that best separates the classes in the feature space.
\item More advanced algorithms like \alert{Random Forests}, \alert{Gradient Boosting}, and \alert{Neural Networks}.
\end{itemize}
\end{definition}
\end{frame}
\section{Classification Basics}
\label{sec:org1b130ea}
\begin{frame}[label={sec:orgd1a15af}]{Logistic Regression}
\begin{definition}[Logistic Regression]\label{sec:orgeff9956}
\pause
\alert{Logistic Regression} models the probability that the target variable \(y\) belongs to a particular class. The \alert{logistic function (sigmoid)} is used to map predicted values to probabilities between 0 and 1. The decision boundary is determined by the threshold (commonly 0.5) for classifying observations into different classes.
\pause

The sigmoid function is defined as:
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$
where \(z\) is a linear combination of the input features.
\end{definition}
\end{frame}
\section{Evaluation Metrics}
\label{sec:org9202193}
\begin{frame}[label={sec:org37d1554}]{Evaluation Metrics for Classification}
\begin{definition}[Confusion Matrix]\label{sec:orge6e091d}
\pause
The \alert{confusion matrix} summarizes the performance of a classification algorithm by comparing predicted and actual labels.
\pause
\begin{center}
\begin{tabular}{lll}
 & \alert{Predicted = 1} & \alert{Predicted = 0}\\
\hline
\alert{Actual = 1} & True Positive (TP) & False Negative (FN)\\
\alert{Actual = 0} & False Positive (FP) & True Negative (TN)\\
\end{tabular}
\end{center}
From these four numbers we obtain the core metrics summarized next.
\end{definition}
\end{frame}
\begin{frame}[label={sec:orgb916c44}]{Accuracy (Overall Success Rate)}
\begin{definition}[Accuracy (Overall Success Rate)]\label{sec:org226961d}
\alert{Accuracy} is a measure of how often the classifier is correct across all predictions. It answers the question: "What fraction of \alert{all} predictions are correct?"
$$\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{FP} + \text{FN} + \text{TN}}$$

Despite its simplicity, accuracy can be misleading, especially in imbalanced datasets where one class dominates.
\end{definition}
\end{frame}
\begin{frame}[label={sec:org4b31bb2}]{Precision (Positive Predictive Value)}
\begin{definition}[Precision (Positive Predictive Value)]\label{sec:org71105d2}
\alert{Precision} is a measure of the accuracy of positive predictions. It answers the question: "When the classifier predicts 1, how often is it correct?"
$$\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}$$

High precision indicates that when the model predicts a positive class, it is likely correct, but it does not account for false negatives.
\end{definition}
\end{frame}
\begin{frame}[label={sec:org7782688}]{Recall (Sensitivity, True-Positive Rate)}
\begin{definition}[Recall (Sensitivity, True-Positive Rate)]\label{sec:org0ba7b5f}
\alert{Recall} is a measure of the model's ability to capture all positive instances. It answers the question: "Of all the actual 1 cases, how many did we catch?"
$$\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}$$
High recall indicates that the model is good at identifying positive cases, but it does not consider false positives.
\end{definition}
\end{frame}
\begin{frame}[label={sec:orgf846ca6}]{F1-Score}
\begin{definition}[F1-Score]\label{sec:org91b589c}
\alert{F1-Score} is the harmonic mean of precision and recall, providing a balance between the two metrics:
$$F_1 = 2 \cdot \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$
It is particularly useful when the class distribution is imbalanced, as it considers both false positives and false negatives.
High F1-Score indicates a good balance between precision and recall, while a low F1-Score suggests that either precision or recall (or both) are low.
\end{definition}
\end{frame}
\begin{frame}[label={sec:orga85a386}]{Precision-Recall Trade-off}
\begin{definition}[Precision-Recall Trade-off]\label{sec:org1c16c72}
The \alert{Precision-Recall Trade-off} illustrates the inverse relationship between precision and recall:
\pause
\begin{itemize}[<+->]
\item Increasing the threshold for classifying a positive instance will \textcolor{darkgreen}{increase precision} but \textcolor{darkred}{decrease recall}.
\item Decreasing the threshold will \textcolor{darkgreen}{increase recall} but \textcolor{darkred}{decrease precision}.
\end{itemize}
\end{definition}
\begin{definition}[Precision-Recall Curve]\label{sec:orge98a3f1}
\pause
\alert{Precision-Recall Curve} is a graphical representation of the trade-off between precision and recall for different threshold values.
\end{definition}
\end{frame}
\begin{frame}[label={sec:org3060e93}]{Receiver Operating Characteristic (ROC) Curve}
\begin{definition}[Receiver Operating Characteristic (ROC) Curve]\label{sec:orgd3ba07a}
\alert{ROC Curve} is a graphical representation of a classifier's performance across different thresholds. It plots the true positive rate (recall) against the false positive rate (FPR) at various threshold settings.
\end{definition}
\end{frame}
\begin{frame}[label={sec:orgfa6511a}]{Area Under the Curve (AUC)}
\begin{definition}[Area Under the Curve (AUC)]\label{sec:org50a2799}
\pause
\alert{Area Under the Curve (AUC)} is a single scalar value that summarizes the performance of a classifier across all possible thresholds. It is calculated as the area under the ROC curve. AUC provides an aggregate measure of performance across all classification thresholds, making it useful for comparing different classifiers.
\end{definition}
\end{frame}
\end{document}
