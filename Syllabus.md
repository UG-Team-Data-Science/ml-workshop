# Session 1: Introduction to Machine Learning


## Working with Jupyter on Habrok

-   Jupyter Notebooks on Habrok
-   Jupyter Notebooks on a local machine
-   Jupyter Notebooks on Google Colab


## What is Machine Learning

-   ML vs. Traditional Programming
-   Key Concepts in Machine Learning
-   Types of Machine Learning


## The Machine Learning Workflow

-   Data Exploration and Preprocessing
-   Model Selection, Training, and Validation
-   Model Evaluation


## Exploratory Data Analysis (EDA)

-   Goals of EDA
-   Typical Techniques


## Train-Test Split and Cross-Validation

-   Train-Test Split
-   Cross-Validation


## Data Preprocessing

-   Handling missing values
-   Feature scaling
-   Encoding categorical variables


## Feature Engineering

-   Polynomial
-   Derived
-   Discretization


# Session 2: Supervised Learning - Regression and Evaluation


## Simple Linear Regression

-   Introduction to Regression
-   Simple Linear Regression
-   Assumptions of Simple Linear Regression
-   Evaluation Metrics


## Robust Regression

-   Types of Robust Regression
-   Huber Robust Regression
-   RANSAC Algorithm


## Multiple Linear Regression

-   Extension of Simple Linear Regression
-   Additional Assumption: Low Multicolinearity


## Regularized Regression

-   Cost Functions
-   Gradient Descent
-   Regularization
-   Ridge and Lasso Regularized Regression


## Nonlinearities in Regression

-   Polynomial Regression
-   Exponential Regression
-   Logarithmic Regression


## Overfitting, Underfitting, and Hyperparameter Tuning

-   Overfitting and Underfitting
-   Bias-Variance Tradeoff
-   Hyperparameter Tuning


# Session 3: Supervised Learning - Classification and Metrics


## Introduction to Classification

-   Binary vs. Multiclass Classification
-   Classification Algorithms
-   Decision Boundaries
-   Probability vs. Hard Classification
-   Output of Classification Models


## Classification Basics

-   Logistic Regression
-   Decision Boundaries


## Evaluation Metrics

-   Confusion Matrix
-   Accuracy, Precision, Recall, F1 Score
-   Precision-Recall Tradeoff and Curve
-   ROC Curve and AUC


## k-Nearest Neighbors (k-NN) and Decision Trees

-   The k-NN Algorithm
-   Decision Trees
-   Information Gain and Gini Index
-   Overfitting and Pruning


## Support Vector Machines (SVMs)

-   Introduction to SVMs
-   Linear SVMs
-   Nonlinear SVMs
-   Kernel Trick


# Session 4: Ensemble Methods


## Introduction to Ensemble Methods

-   What are Ensemble Methods?
-   Why Use Ensemble Methods?
-   Types of Ensemble Methods


## Bagging and Random Forests

-   Bagging (Bootstrap Aggregating) Overview
-   Random Forests
-   Feature Importance in Random Forests


## Boosting

-   Boosting Overview
-   AdaBoost
-   Gradient Boosting
-   XGBoost


## Ensembles of Ensembles

-   Stacking
-   Multi-level EoE


# Session 5: Unsupervised Learning - Clustering and Dimensionality Reduction


## Introduction to Unsupervised Learning

-   Dimensionality Reduction
-   Clustering
-   Anomaly Detection


## Dimensionality Reduction

-   Principal Component Analysis (PCA)
-   Autoencoders
-   Linear Discriminant Analysis (LDA)


## Clustering

-   k-Means Clustering
-   Hierarchical Clustering


## Anomaly Detection


# Session 6: Artificial Neural Networks and Deep Learning


## Introduction to Neural Networks

-   What are Neural Networks?
-   Biological Inspiration
-   Structure of a Neural Network
-   Activation Functions


## Training Neural Networks

-   Forward Propagation
-   Backpropagation
-   Loss Functions
-   Gradient Descent


## Deep Learning

-   What is Deep Learning?
-   Deep Neural Networks
-   Convolutional Neural Networks (CNNs)
-   Recurrent Neural Networks (RNNs)
-   Autoencoders
-   Generative Adversarial Networks (GANs)
