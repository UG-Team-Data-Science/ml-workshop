{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7080c6b",
   "metadata": {},
   "source": [
    "\n",
    "# Overview of Unsupervised Learning\n",
    "\n",
    "**Unsupervised learning** is a type of machine learning where the model learns patterns from unlabeled data. Unlike supervised learning, where the model is trained on labeled examples, unsupervised learning algorithms try to infer the underlying structure of the data without any explicit labels. This makes it particularly useful for exploratory data analysis, clustering, and dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d35e1ce",
   "metadata": {},
   "source": [
    "## Types of Unsupervised Learning\n",
    "\n",
    "Unsupervised learning can be broadly categorized into two main types:\n",
    "\n",
    "-   **Dimensionality Reduction**: Reducing the number of features while preserving the essential structure of the data.\n",
    "-   **Clustering**: Grouping similar data points together based on their features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23333c60",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "**Dimensionality reduction** techniques aim to reduce the number of features in a dataset while retaining its essential characteristics. This is particularly useful for visualizing high-dimensional data or improving the performance of machine learning models by reducing noise and computational complexity. Common dimensionality reduction techniques include:\n",
    "\n",
    "-   **Principal Component Analysis (PCA)**: Projects the data onto a lower-dimensional space by maximizing the variance along the new axes (principal components).\n",
    "-   **t-Distributed Stochastic Neighbor Embedding (t-SNE)**: A non-linear technique that visualizes high-dimensional data by minimizing the divergence between probability distributions of pairwise similarities in high and low dimensions.\n",
    "-   **Autoencoders**: Neural networks that learn to encode the input data into a lower-dimensional representation and then decode it back to the original space. They are particularly useful for learning complex, non-linear mappings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf05de",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "**Clustering** is often used to discover inherent groupings in data, such as customer segmentation in marketing or grouping similar documents in text analysis. Common clustering algorithms include:\n",
    "\n",
    "-   **K-Means**: Partitions data into K clusters by minimizing the variance within each cluster.\n",
    "-   **Hierarchical Clustering**: Builds a tree of clusters by either merging or splitting clusters based on distance metrics.\n",
    "-   **DBSCAN**: Groups together points that are closely packed together while marking points that lie alone in low-density regions as outliers.\n",
    "-   **Gaussian Mixture Models (GMM)**: Assumes that the data is generated from a mixture of several Gaussian distributions and uses the Expectation-Maximization algorithm to estimate the parameters of these distributions.\n",
    "\n",
    "****Advantages and Disadvantages****\n",
    "\n",
    "-   ****Advantages****:\n",
    "    -   Can discover hidden patterns and structures in data.\n",
    "    -   Useful for exploratory data analysis and feature engineering.\n",
    "    -   Can handle large datasets without the need for labeled data.\n",
    "-   ****Disadvantages****:\n",
    "    -   Results can be difficult to interpret, especially in clustering.\n",
    "    -   No guarantee of finding meaningful patterns; results can be sensitive to the choice of algorithm and parameters.\n",
    "    -   Evaluation of clustering results can be challenging without ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e3c86",
   "metadata": {},
   "source": [
    "## Practical Demonstration\n",
    "\n",
    "To illustrate the concepts of unsupervised learning, we will use the Iris dataset and apply PCA for dimensionality reduction and K-Means for clustering.\n",
    "\n",
    "-   Load the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e480ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the Iris dataset\n",
    "df = sns.load_dataset('iris')\n",
    "X = df.drop(columns=['species'])\n",
    "y = df['species'].astype('category').cat.codes\n",
    "\n",
    "sns.pairplot(df, hue='species', palette='viridis', markers=[\"o\", \"s\", \"D\"])\n",
    "plt.suptitle('Iris Dataset Pairplot', y=1.02)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Features')\n",
    "# plt.legend(title='Species', loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ae8556",
   "metadata": {},
   "source": [
    "-   Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feea03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "\n",
    "# Standardize the data\n",
    "scaler = Normalizer()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c563776",
   "metadata": {},
   "source": [
    "-   Apply PCA for dimensionality reduction to 2 dimensions and visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2, n_oversamples=100, random_state=42)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot the PCA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.title('PCA of Iris Dataset')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b84379",
   "metadata": {},
   "source": [
    "-   Apply K-Means clustering on the PCA-reduced data and visualize the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, max_iter=10, random_state=42)\n",
    "kmeans.fit(X_pca)\n",
    "y_kmeans = kmeans.predict(X_pca)\n",
    "\n",
    "# Plot the K-Means clustering results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_kmeans, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "            s=150, c='red', marker='X', label='Centroids')\n",
    "plt.title('K-Means Clustering on PCA-reduced Data')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a563bb",
   "metadata": {},
   "source": [
    "-   Evaluate the clustering results using silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate silhouette score\n",
    "silhouette_avg = silhouette_score(X_pca, y_kmeans)\n",
    "print(f'Silhouette Score: {silhouette_avg:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d538c",
   "metadata": {},
   "source": [
    "-   Visualize the decision boundary of K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ae25a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    kmeans, X_pca, response_method=\"predict\", cmap='viridis', alpha=0.8,\n",
    "    grid_resolution=5000, xlabel='Principal Component 1', ylabel='Principal Component 2'\n",
    ")\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y_kmeans, palette='viridis', edgecolor='k', s=50)\n",
    "plt.xlim(-0.5, 0.5)\n",
    "plt.ylim(-0.1, 0.1)\n",
    "plt.title('Decision Boundary of K-Means Clustering')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1e0cbf",
   "metadata": {},
   "source": [
    "## Hands-on Exercises\n",
    "\n",
    "Apply PCA and K-Means clustering on a different dataset, such as the Wine dataset or the Breast Cancer dataset from scikit-learn. Visualize the results and interpret the clusters formed.\n",
    "\n",
    "-   Load the dataset\n",
    "-   Explore the dataset\n",
    "-   Visualize the dataset\n",
    "-   Preprocess the data\n",
    "-   Apply PCA to reduce to 2 dimensions\n",
    "-   Apply K-Means clustering and plot the results"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "ML Workshop",
   "language": "python",
   "name": "ml_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
