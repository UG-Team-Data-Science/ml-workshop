{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed72b66",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "**Multiple linear regression** is an extension of simple linear regression that allows us to model the relationship between a dependent variable and **multiple** independent variables. It is used when we want to predict a continuous outcome based on several predictors. The model assumes that the relationship between the dependent variable and the independent variables can be expressed as a linear equation: $$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon$$ where:\n",
    "\n",
    "-   $y$ is the dependent variable (the outcome we want to predict).\n",
    "-   $\\beta_0$ is the y-intercept (the value of $y$ when all $x_i$ are 0).\n",
    "-   $\\beta_1, \\beta_2, ..., \\beta_n$ are the coefficients for each independent variable $x_i$ (the change in $y$ for a one-unit change in $x_i$).\n",
    "-   $x_1, x_2, ..., x_n$ are the independent variables (the predictors).\n",
    "-   $\\epsilon$ is the error term (the difference between the observed and predicted values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba42ff5a",
   "metadata": {},
   "source": [
    "## Assumptions of Multiple Linear Regression\n",
    "\n",
    "The assumptions of multiple linear regression are similar to those of simple linear regression, but with some additional considerations due to the presence of multiple predictors:\n",
    "\n",
    "1.  **Linearity**: The relationship between the dependent variable and each independent variable is linear.\n",
    "2.  **Independence**: The observations are independent of each other.\n",
    "3.  **Homoscedasticity**: The variance of the errors is constant across all levels of the independent variables.\n",
    "4.  **Normality of errors**: The errors (residuals) are normally distributed.\n",
    "5.  **No multicollinearity**: The independent variables are not too highly correlated with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77328f46",
   "metadata": {},
   "source": [
    "## Practical Demonstration\n",
    "\n",
    "We will use a feature of `scikit-learn` to generate a synthetic dataset for linear regression. This will allow us to demonstrate the concepts of linear regression without focusing on data preprocessing or feature engineering.\n",
    "\n",
    "`scikit-learn` provides a simple way to create synthetic datasets for regression tasks, which can be useful for testing and learning purposes. We will use the `make_regression` function to generate a dataset with a specified number of samples, features, and noise level.\n",
    "\n",
    "-   Generate a synthetic dataset using the `make_regression` function from `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e3f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y, coef = make_regression(n_samples=100,\n",
    "                             n_features=2,\n",
    "                             bias=1, noise=10,\n",
    "                             coef=True, random_state=42)\n",
    "# Print the coefficients\n",
    "print(\"True coefficients:\", coef)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(X, columns=['Feature 1', 'Feature 2'])\n",
    "df['Target'] = y\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff38655",
   "metadata": {},
   "source": [
    "-   Explore the dataset and create a correlation matrix to visualize the relationships between the features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d865cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Features and Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2966c9",
   "metadata": {},
   "source": [
    "-   Perform the train-test split to prepare the data for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b47095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c68a506",
   "metadata": {},
   "source": [
    "-   Fit a linear regression model to the training data and make predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454edbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "# Print the coefficients and intercept\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b51ea7b",
   "metadata": {},
   "source": [
    "-   Visualize the predictions vs actual values, the residuals, and the residuals vs the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89952d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c598ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the residuals\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title('Residuals Distribution')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891e90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the residuals vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(0, color='k', linestyle='--', lw=2)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afa4958",
   "metadata": {},
   "source": [
    "-   Evaluate the model using metrics such as Mean Squared Error (MSE) and R-squared ($R^2$) score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd89d618",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R^2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f7f041",
   "metadata": {},
   "source": [
    "## Hands-on Exercises\n",
    "\n",
    "-   Generate a synthetic dataset using the `make_regression` function from `scikit-learn`; use the following parameters:\n",
    "    -   `n_samples` = 200\n",
    "    -   `n_features` = 3\n",
    "    -   `noise` = 10\n",
    "    -   `random_state` = 42\n",
    "    -   `bias` = 2\n",
    "-   Explore the dataset and create a correlation matrix to visualize the relationships between the features and the target variable.\n",
    "-   Train a linear regression model on the synthetic dataset; choose two features from the dataset as independent variables and the target variable as the dependent variable.\n",
    "-   Split the dataset into training and testing sets (80% train, 20% test).\n",
    "-   Fit a linear regression model to the training data.\n",
    "-   Print the coefficients and intercept of the model.\n",
    "-   Make predictions on the `test` dataset.\n",
    "-   Calculate and print the Mean Squared Error (MSE) and R-squared ($R^2$) score of the model on the test set.\n",
    "-   Visualize the predictions vs actual values, the residuals, and the residuals vs the predicted values.\n",
    "-   Modify the synthetic dataset by introducing a non-linear relationship (e.g., quadratic or exponential) and observe how the performance metrics change. Fit a simple linear regression model to this new dataset and evaluate its performance."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "ML Workshop",
   "language": "python",
   "name": "ml_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
