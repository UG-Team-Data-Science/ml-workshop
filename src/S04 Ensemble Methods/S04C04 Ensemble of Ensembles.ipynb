{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "829d4723",
   "metadata": {},
   "source": [
    "\n",
    "# Ensemble of Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b9e3fd",
   "metadata": {},
   "source": [
    "## Ensemble of Ensembles\n",
    "\n",
    "Ensemble of Ensembles is a meta-ensemble method that combines multiple ensemble methods to improve predictive performance. It leverages the strengths of different ensemble techniques, such as bagging, boosting, and stacking, to create a more robust model. The idea is to use the predictions from various ensemble methods as inputs to a final model, which can be another ensemble method or a simple model like logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205b7fc8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Advantages of Ensemble of Ensembles\n",
    "\n",
    "-   **Improved Accuracy**: By combining predictions from multiple ensemble methods, the overall accuracy can be significantly improved.\n",
    "-   **Robustness**: It reduces the risk of overfitting by averaging out the predictions from different models, leading to better generalization.\n",
    "-   **Flexibility**: It allows the use of diverse ensemble methods, which can capture different patterns in the data.\n",
    "-   **Diversity**: The combination of different ensemble methods can lead to a more diverse set of predictions, which is beneficial for model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416cb66b",
   "metadata": {},
   "source": [
    "## Disadvantages of Ensemble of Ensembles\n",
    "\n",
    "-   **Complexity**: The model can become complex and harder to interpret, as it involves multiple layers of models.\n",
    "-   **Computational Cost**: Training multiple ensemble methods can be computationally expensive and time-consuming.\n",
    "-   **Risk of Overfitting**: If not managed properly, combining too many models can lead to overfitting, especially if the base models are too complex.\n",
    "-   **Hyperparameter Tuning**: Each ensemble method may require its own hyperparameter tuning, adding to the complexity of the model development process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b1230c",
   "metadata": {},
   "source": [
    "## Types of Ensemble of Ensembles\n",
    "\n",
    "Ensemble of Ensembles can be implemented using various ensemble methods, including:\n",
    "\n",
    "-   **Stacking**: Combines predictions from multiple base models using a meta-model that learns how to best combine their outputs (e.g., logistic regression, random forest).\n",
    "-   **Multi-level Stacking**: A more complex version of stacking where multiple layers of models are trained, and predictions from one layer are used as inputs to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa896a4",
   "metadata": {},
   "source": [
    "### Example: Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4739d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "titanic = datasets.fetch_openml('titanic', version=1, as_frame=True)\n",
    "df = titanic.frame[['pclass', 'sex', 'age', 'survived']]\n",
    "X = df.drop(columns=['survived'])\n",
    "y = df['survived'].astype('category').cat.codes\n",
    "class_names = ['Not Survived', 'Survived']\n",
    "feature_names = X.columns.tolist()\n",
    "print(f\"Feature names: {feature_names}\")\n",
    "print(f\"Class labels: {class_names}\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "preprocessing_pipeline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('age', Pipeline(\n",
    "            steps=[\n",
    "                ('imputer', SimpleImputer(strategy='mean')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]\n",
    "        ), ['age']),\n",
    "        ('sex', Pipeline(\n",
    "            steps=[\n",
    "                ('encoder', OneHotEncoder(drop='first', sparse_output=False))\n",
    "            ]\n",
    "        ), ['sex']),\n",
    "        ('pclass', Pipeline(\n",
    "            steps=[\n",
    "                ('enconder', OneHotEncoder(sparse_output=False))\n",
    "            ]\n",
    "        ), ['pclass'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "X_train_transformed = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_test_transformed = preprocessing_pipeline.transform(X_test)\n",
    "\n",
    "feature_names = preprocessing_pipeline.get_feature_names_out()\n",
    "print(f\"Transformed feature names: {feature_names}\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100))\n",
    "]\n",
    "# Define the final model\n",
    "final_model = LogisticRegression()\n",
    "# Create the stacking ensemble\n",
    "stacking_ensemble = StackingClassifier(estimators=base_models, final_estimator=final_model)\n",
    "# Fit the stacking ensemble\n",
    "stacking_ensemble.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7232641",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Evaluate the stacking ensemble\n",
    "accuracy = stacking_ensemble.score(X_test_transformed, y_test)\n",
    "print(f\"Stacking Ensemble Accuracy: {accuracy:.2f}\")\n",
    "# Predict using the stacking ensemble\n",
    "y_pred = stacking_ensemble.predict(X_test_transformed)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed570c2",
   "metadata": {},
   "source": [
    "### Example: Multi-level Ensemble of Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2778445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define base models for the first level\n",
    "base_models_level_1 = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100)),\n",
    "    ('svc', SVC(probability=True))\n",
    "]\n",
    "# Create the first level ensemble using voting\n",
    "first_level_ensemble = VotingClassifier(estimators=base_models_level_1, voting='soft')\n",
    "# Fit the first level ensemble\n",
    "first_level_ensemble.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Define base models for the second level\n",
    "base_models_level_2 = [\n",
    "    ('first_level', first_level_ensemble),\n",
    "    ('log_reg', LogisticRegression())\n",
    "]\n",
    "# Create the second level ensemble using stacking\n",
    "second_level_ensemble = StackingClassifier(estimators=base_models_level_2, final_estimator=LogisticRegression())\n",
    "# Fit the second level ensemble\n",
    "second_level_ensemble.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the second level ensemble\n",
    "accuracy_level_2 = second_level_ensemble.score(X_test_transformed, y_test)\n",
    "print(f\"Multi-level Ensemble of Ensembles Accuracy: {accuracy_level_2:.2f}\")\n",
    "# Predict using the second level ensemble\n",
    "y_pred_level_2 = second_level_ensemble.predict(X_test_transformed)\n",
    "print(classification_report(y_test, y_pred_level_2, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6c6674",
   "metadata": {},
   "source": [
    "In this example, we first create a first-level ensemble using a voting classifier that combines predictions from a random forest, gradient boosting, and support vector machine. Then, we use the predictions from this first-level ensemble as input to a second-level stacking ensemble that combines it with logistic regression. This multi-level approach allows us to leverage the strengths of different ensemble methods at multiple levels, potentially improving predictive performance."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "ML Workshop",
   "language": "python",
   "name": "ml_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
