{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e34019",
   "metadata": {},
   "source": [
    "\n",
    "# Classification Basics\n",
    "\n",
    "We will now explore some basic algorithms used in classification tasks, focusing on logistic regression, which is a foundational algorithm in this area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee26b82",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Despite its name, logistic regression is a classification algorithm. It models the probability that the target variable $y$ belongs to a particular class. The [logistic function (sigmoid)](https://en.wikipedia.org/wiki/Logistic_function) is used to map predicted values to probabilities between 0 and 1. The decision boundary is determined by the threshold (commonly 0.5) for classifying observations into different classes.\n",
    "\n",
    "The sigmoid function is defined as: $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$ where $z$ is a linear combination of the input features. In a one-dimensional case, it can be visualized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97de385",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "z = np.linspace(-10, 10, 100)\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "plt.plot(z, sigmoid)\n",
    "plt.title(\"Sigmoid Function\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"Ïƒ(z)\")\n",
    "plt.grid()\n",
    "plt.axhline(0.5, color='red', linestyle='--', label='Threshold (0.5)')\n",
    "plt.axvline(0, color='green', linestyle='--', label='Decision Boundary (z=0)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecef3f11",
   "metadata": {},
   "source": [
    "### Mathematical Formulation\n",
    "\n",
    "The logistic regression model can be expressed as: $$P(y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_n X_n)}}$$ where\n",
    "\n",
    "-   $P(y = 1|X)$: probability of the positive class given the features $X$\n",
    "-   $\\beta_0, \\beta_1, \\ldots, \\beta_n$: the model coefficients\n",
    "\n",
    "The logistic regression model is trained by maximizing the likelihood of the observed data, which is equivalent to minimizing the negative log-likelihood (cross-entropy loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc25177",
   "metadata": {},
   "source": [
    "## Practical Demonstration\n",
    "\n",
    "To illustrate logistic regression, we will use a very simple synthetic dataset, consisting of one feature and a binary target variable. This will allow us to visualize the decision boundary clearly.\n",
    "\n",
    "-   Generating a synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88516595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "# Generate a synthetic dataset with one feature and binary target\n",
    "X, y = make_classification(n_samples=1000, n_features=1, n_informative=1, n_redundant=0,\n",
    "                           n_clusters_per_class=1, random_state=0)\n",
    "df = pd.DataFrame(X, columns=['feature'])\n",
    "df['target'] = y\n",
    "df['target'] = df['target'].astype('category')\n",
    "df['target'] = df['target'].cat.codes\n",
    "\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be952a12",
   "metadata": {},
   "source": [
    "-   Visualizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dc6287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(data=df, x='feature', y='target', hue='target', palette='Set1')\n",
    "plt.title(\"Synthetic Dataset\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")\n",
    "# plt.axhline(0.5, color='red', linestyle='--', label='Decision Boundary (0.5)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f5e844",
   "metadata": {},
   "source": [
    "-   Training a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ccb9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(df[['feature']], df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90be79a5",
   "metadata": {},
   "source": [
    "-   Evaluating the model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56630bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model accuracy:\", accuracy)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e1f309",
   "metadata": {},
   "source": [
    "-   Visualizing the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d95a2ad",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "x_range = np.linspace(X['feature'].min(), X['feature'].max(), 100).reshape(-1, 1)\n",
    "X_range = pd.DataFrame(x_range, columns=X.columns)\n",
    "y_prob = clf.predict_proba(X_range)[:, 1]\n",
    "\n",
    "plt.scatter(X_test['feature'], y_test, c='blue', label='Test Data', alpha=0.5)\n",
    "plt.plot(X_range, y_prob, color='red', label='Probability of Class 1')\n",
    "plt.axvline(X_range[y_prob < 0.5]['feature'].max(),\n",
    "            color='green', linestyle='--',\n",
    "            label='Decision Boundary (Threshold = 0.5)')\n",
    "plt.axhline(0.5, color='black', linestyle='--', label='Threshold (0.5)')\n",
    "plt.title(\"Logistic Regression Decision Boundary\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Probability of Class 1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84ad3b1",
   "metadata": {},
   "source": [
    "## Hands-on Exercises\n",
    "\n",
    "Modify the synthetic dataset to include two features instead of one. Train a logistic regression model and visualize the decision boundary in a 2D plot.\n",
    "\n",
    "-   Generate the 2D synthetic dataset.\n",
    "-   Visualize the dataset.\n",
    "-   Split the dataset into training and test sets.\n",
    "-   Train a logistic regression model.\n",
    "-   Evaluate the model's accuracy.\n",
    "-   Visualize the decision boundary."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "ML Workshop",
   "language": "python",
   "name": "ml_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
